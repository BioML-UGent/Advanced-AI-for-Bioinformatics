{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50f7fba6",
   "metadata": {},
   "source": [
    "<img src='https://analyticsindiamag.com/wp-content/uploads/2022/04/Screenshot-2022-04-06-at-9.55.11-PM.png' width=750>\n",
    "\n",
    "*Images generated by a deep neural network that interprets text to generate images (DALL·E 2)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfae31e",
   "metadata": {},
   "source": [
    "# PC lab: intro to Neural networks & PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b169ea",
   "metadata": {},
   "source": [
    "Deep learning is the subfield of machine learning that concerns neural networks with representation learning capabilities. As of recent years, it is arguably the most quickly growing field within machine learning, enjoying major breakthroughs every year (Listing a couple ones from last year(s): ChatGPT, AlphaFold v2, DALL·E 2, AlphaZero). Although the popularity of neural nets is a recent phenomenon, they were first described by Warren McCulloch and Walter Pitts in 1943. Early progress in training competitive neural networks was stalled by a multitude of reasons, such as the limited computer resources, sub-optimal network architectures and the use of smaller datasets. In this PC-lab we will introduce you to the basics of implementing a neural network using contemporary practices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463cea2",
   "metadata": {},
   "source": [
    "## 1 Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d9f36",
   "metadata": {},
   "source": [
    "### From linear models to neurons to neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54393fb5",
   "metadata": {},
   "source": [
    "The core unit of every (artificial) neural network is considered the neuron. Every neuron can be observed as a linear combination of **one or more inputs** $\\mathbf{x}$ with weights $\\mathbf{w}$ (and optionally adding a **bias** $w_0$), outputting a **single output** $a_j$ (j for the j-th neuron in a layer):\n",
    "\n",
    "\n",
    "$a_j = \\sum\\limits_{i=1}^{D}(w_{ji}x_i) + w_0$\n",
    "\n",
    "We can absorb the bias into the weights by adding an extra input $x_0 = 1$. This gives us:\n",
    "\n",
    "$a_j = \\sum\\limits_{i=0}^{D}(w_{ji}x_i)$\n",
    "\n",
    "To obtain the *deep learning magic*, we need to make the whole thing **non-linear**. We do this by adding **activation functions** after every (hidden) layer. The most classical activation is the sigmoid activation $\\sigma()$, used also in logistic regression. Nowadays, we usually opt for a more simple activation function: the **ReLU** $\\texttt{ReLU}(z) = max(0,z)$. This function has the favorable property that its derivative is very efficient to compute (1 when $z$ is positive, 0 when it is negative). It also acts as a switch: a neuron will have a \"dead\" ($0$) activation whenever $z$ is negative. We can write this non-linear activation as:\n",
    "\n",
    "$z_j = h(a_j)$\n",
    "\n",
    "Or more generally for layer $l$:\n",
    "\n",
    "$z^{(l)} = h^{(l)}(W^{(l)}z^{(l-1)})$\n",
    "\n",
    "\n",
    "\n",
    "For the output layer of a neural network: our activation depends on the task at hand. For binary classification, we use sigmoid to constrain our output between 0 and 1. For multi-class, we use a softmax operation so the output of all neurons sums to 1. For regression, we simply do not use an activation (or a custom one depending on your data: if you already know that your outputs can't be negative but can take all positive numbers ($\\mathbb{R}^+$), then maybe a ReLU activation in the output nodes makes sense)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038a1cb3",
   "metadata": {},
   "source": [
    "In order to build more intuition for neural networks: consider the following figure where we \"visually build up\" a neural network starting from Linear regression with four input features (**a**), to Logistic regression (**b**), to an archetypical output neuron with ReLU activation (**c**). For multi-output settings, we visualize multi-output regression (**d**), multi-label classification (more than one class can be 1 in a sample) (**e**), and multi-class classification via softmax (**f**). Finally, a simple neural network with two hidden layers for binary classification (sigmoid output head) is shown under **g**.\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/BioML-UGent/MLLS/main/11_intro_nns/lr2nn.png'>\n",
    "\n",
    "**This figure makes it crystal clear that the most simple neural network is just a bunch of linear regressions stacked on top of eachother with non-linearities inbetween.** More advanced neural network architectures exist that modify how we make information flow between inputs. In this example, everything is just connected to everything with linear weights. This type of neural network is what we call an **MLP** or a **multi layer perceptron**.\n",
    "\n",
    "\n",
    "\n",
    "<!-- TEX CODE TO GENERATE THE ABOVE:\n",
    "\\begin{table}[]\n",
    "\\resizebox{1.00\\linewidth}{!}{\n",
    "\\begin{tabular}{llll}\n",
    "\\toprule\n",
    "\\textbf{Loss}    & \\textbf{Formula}     & \\textbf{Purpose}        & \\textbf{Domain} \\\\[1mm] \\midrule\n",
    "Squared loss & $\\mathcal{C}(y,~f(\\mathbf{x}))~=~(y-f(\\mathbf{x}))^2$ & Regression & $f(\\mathbf{x}) \\in \\mathbb{R}$ \\\\[2mm]\n",
    "Logistic loss & $\\mathcal{C}(y,~f(\\mathbf{x}))~= -y\\log(f(\\mathbf{x}))-(1-y)\\log(1-f(\\mathbf{x}))$ & Binary clf & $f(\\mathbf{x}) \\in [0,1]$ \\\\[2mm]\n",
    "Cross entropy & $\\mathcal{C}(\\mathbf{y},~\\mathbf{f}(\\mathbf{x}))~= - \\sum_{j=1}^k y_j\\log(f_j(\\mathbf{x})) $ & \\begin{tabular}[c]{@{}l@{}}Multi-class\\\\ classification\\end{tabular} & $\\mathbf{f}(\\mathbf{x}) \\in \\Delta^k$ \\\\[3.5mm]\n",
    "Multi-label logistic loss & $\\mathcal{C}(\\mathbf{y},~\\mathbf{f}(\\mathbf{x}))~= - \\sum_{j=1}^k(-y_j\\log(f_j(\\mathbf{x}))-(1-y_j)\\log(1-f_j(\\mathbf{x}))) $ & \\begin{tabular}[c]{@{}l@{}}Multi-label\\\\ classification\\end{tabular} & $\\mathbf{f}(\\mathbf{x}) \\in [0,1]^k$ \\\\[3.5mm]\n",
    "Multi-output squared loss & $\\mathcal{C}(\\mathbf{y},~\\mathbf{f}(\\mathbf{x}))~= - \\sum_{j=1}^k(y_j-f_j(\\mathbf{x}))^2 $ & \\begin{tabular}[c]{@{}l@{}}Multi-output\\\\ regression\\end{tabular} & $\\mathbf{f}(\\mathbf{x}) \\in \\mathbb{R}^k$ \\\\[2mm]\\bottomrule\n",
    "\\end{tabular}\n",
    "}\n",
    "\\end{table}\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8fcd44",
   "metadata": {},
   "source": [
    "Keep in mind that all of the above methods usually fit a bias/intercept in addition to weights fitted on the input features.\n",
    "For an MLP, visually it would look like this:\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/BioML-UGent/MLLS/main/11_intro_nns/biases.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b976b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>THOUGHT EXERCISE:</b>\n",
    "<p> How much weights does the model pictured above have (including biases)? </p>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36424acf",
   "metadata": {},
   "source": [
    "Written answer:\n",
    "The network goes from 4->3->2->1 neurons.\n",
    "\n",
    "The first layer has (4+1)\\*3 weights.\n",
    "The next (3+1)\\*2.\n",
    "The last (2+1)\\*1.\n",
    "In total: 15+8+3=26"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5d8cdf",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4414627f",
   "metadata": {},
   "source": [
    "Dropout is a popular addition to neural networks. It is a form of regularization during which we stochastically deactivate a percentage of neurons in every training step by putting their activation to zero. This regularization only happens during training, as during testing we (usually) want deterministic outputs. Conceptually, it is similar to other regularization techniques such as ridge regression and subsampling of features in random forest training, in the sense that it will force our model to look at all features, because sometimes one single feature will not be available during training. The difference here is that we do it by stochastically putting nodes to zero during training, and that we can perform it not only on our input features, but also on our hidden nodes.\n",
    "\n",
    "Mathematically, it can be performed by simply sampling a boolean vector and doing element-wise multiplication.\n",
    "\n",
    "Visually, it would look a bit like this, where nodes in cyan are dropped out, and the associated cyan weights do not have any influence on training anymore (in that training step):\n",
    "\n",
    "<img src='https://raw.githubusercontent.com/BioML-UGent/MLLS/main/11_intro_nns/dropout.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0262f0b5",
   "metadata": {},
   "source": [
    "## 2 PyTorch\n",
    "\n",
    "To implement neural networks with more ease, a few high-level python libraries are available: (PyTorch, TensorFlow/keras, JAX, ...). These libraries provide functionality in terms of automatic differentiation (backprop), ready-to-use implementations for various layers, loss functions ...\n",
    "\n",
    "In this lab, we will use [PyTorch](https://pytorch.org). PyTorch is the most popular library for deep learning in academia as of today. For this course it offers the advantage that it has the most 'pythonic' syntax, to the point where almost all NumPy functions have a PyTorch counterpart.\n",
    "\n",
    "If you want to run this notebook locally, you can find the installation instructions for PyTorch [here](https://pytorch.org/get-started/locally/). Make sure to select the right installation options depending on your system (if you have a GPU or not).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1df2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17928e4e",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "Tensors are the fundamental data structures in PyTorch. They are analogous to NumPy arrays. The difference is that tensors can also run on GPU hardware. GPU hardware is optimized for many small computations. Matrix multiplications, the building blocks of all deep learning, run orders-of-magnitude faster on GPU than on CPU. Let's see how tensors are constructed and what we can do with them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfdda56",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [[5,8],[9,8]]\n",
    "print(torch.tensor(x))\n",
    "print(np.array(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf252fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_numpy = np.array(x)\n",
    "print(torch.from_numpy(x_numpy))\n",
    "\n",
    "x_torch = torch.tensor(x)\n",
    "print(x_torch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dcab4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.randn(8).shape)\n",
    "print(np.random.randn(8,50).shape)\n",
    "\n",
    "\n",
    "print(torch.randn(8).shape) # an alternative for .shape in PyTorch is .size()\n",
    "print(torch.randn(8,50).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720657e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.zeros((8,50)).shape)\n",
    "print(torch.zeros(8,50).shape) # works with 'ones' as well"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff1d95c",
   "metadata": {},
   "source": [
    "In PyTorch, the standard data type for floats is `float32`, which is synonymous to `float` within its framework. `float64` is synonymous to `double`.\n",
    "This is different from the NumPy defaults and naming conventions: NumPy default data type for float is `float64`. Keep this in mind when converting numpy arrays to tensors and back!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a729f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.zeros(8).dtype)\n",
    "print(torch.zeros(8).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd4bf7",
   "metadata": {},
   "source": [
    "Conversion of data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d94b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8)\n",
    "print(x.dtype)\n",
    "x = x.to(torch.float64)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90176a8f",
   "metadata": {},
   "source": [
    "`torch.long` is synonymous to `torch.int64`. The only difference between int32 and int64 is the amount of bytes with which you will store every integer. If you go up to very high numbers, you will get numerical overflow faster with more compressed data types. We recommend you to always use the defaults: `torch.long` and `torch.float`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95a6290",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(low=0, high=8, size=(8,), dtype=torch.int32)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "x = x.to(torch.long)\n",
    "print(x.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b2e150",
   "metadata": {},
   "source": [
    "Indexing and other operations work as in NumPy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ebf9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8,50,60)\n",
    "print(x.shape)\n",
    "print(x[:4,10:-10].shape)\n",
    "x[0,0,:10] = 0\n",
    "print(x[0,0,:16])\n",
    "\n",
    "print(torch.min(x), torch.max(x), torch.min(torch.abs(x)))\n",
    "# most of these functions are also tensor methods:\n",
    "print(x.min(), x.max(), x.abs().min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2694ef",
   "metadata": {},
   "source": [
    "Joining tensors via concatenation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd113a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "x_cat0 = torch.cat([x, x], dim=0)\n",
    "print(x_cat0.shape)\n",
    "x_cat1 = torch.cat([x, x, x], dim=1)\n",
    "print(x_cat1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47e391b",
   "metadata": {},
   "source": [
    "Matrix multiplication: let's say we have an input `x`, consisting of 8 samples with 26 features, that we linearly combine with weights `w` to get a single output for every sample:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3939159",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8,26)\n",
    "w = torch.randn(26,1)\n",
    "\n",
    "y_hat = torch.matmul(x, w) # an alternative and equivalent syntax is x @ w\n",
    "print(y_hat)\n",
    "print(y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc341ba7",
   "metadata": {},
   "source": [
    "Note that matrix multiplication is different from element-wise multiplication. For element-wise, `*` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39dedfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(8)\n",
    "print(x)\n",
    "x = x - 1.5\n",
    "print(x)\n",
    "x -= 1.5\n",
    "print(x)\n",
    "x += torch.randn(1)\n",
    "print(x)\n",
    "x += torch.randn(8)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf393ec3",
   "metadata": {},
   "source": [
    "Broadcasting works as in NumPy: [link](https://pytorch.org/docs/stable/notes/broadcasting.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e9002a",
   "metadata": {},
   "source": [
    "Keep in mind, just like in NumPy, whatever you want to do with a tensor, there's probably an elegant operation for it implemented somewhere, you just have to look for it (on google and in the documentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d329a7a9",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE:</b>\n",
    "<p> a) Create a tensor that represents a dataset with 20 samples with 15 observed features. The first 10 features should be random values between 0 and 1, while the last 5 should be sampled from a normal distribution with mean 0 and variance 1.</p>\n",
    "\n",
    "b) Calculate the mean and standard deviation for every feature (column) in the dataset you created in a).\n",
    "\n",
    "c) Split the dataset in a training set (16 samples) and a test set (4 samples).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9292b076",
   "metadata": {},
   "source": [
    "a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac1f221",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## YOUR CODE HERE #########\n",
    "\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cdf6f9",
   "metadata": {},
   "source": [
    "b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aac5563",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## YOUR CODE HERE #########\n",
    "\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372570c0",
   "metadata": {},
   "source": [
    "c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a013c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## YOUR CODE HERE #########\n",
    "\n",
    "#################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11761825",
   "metadata": {},
   "source": [
    " ## 3 Building a neural network in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dcc381",
   "metadata": {},
   "source": [
    "### 3.1 The building blocks\n",
    "\n",
    "Neural networks are initialized through the use of class objects.\n",
    "\n",
    "Many of the functionalities necessary to create [**all types of neural networks**](http://www.asimovinstitute.org/neural-network-zoo/) have [**already been implemented**](http://pytorch.org/docs/master/nn.html).\n",
    "\n",
    "Let's inspect the most basic building blocks first: the [linear layer](https://pytorch.org/docs/master/generated/torch.nn.Linear.html#torch.nn.Linear) and the [ReLU](https://pytorch.org/docs/master/generated/torch.nn.ReLU.html#torch.nn.ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aae5913",
   "metadata": {},
   "source": [
    "A linear layer is an object that will perform a matrix multiplication once called. Here, we instantiate such a layer with 20 input nodes and 40 output nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd11fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "nn.Linear(20, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdea693e",
   "metadata": {},
   "source": [
    "Let's simulate some random data for this layer: A data set (or batch) with 16 samples and 20 features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8108b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(16, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc882b5c",
   "metadata": {},
   "source": [
    "Now let's use our linear layer on this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e43e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(20, 40)\n",
    "print(x.shape)\n",
    "z = layer(x)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3353e4",
   "metadata": {},
   "source": [
    "What happens when we try to feed our layer an input with a different number of features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1494ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(16, 30)\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2832914a",
   "metadata": {},
   "source": [
    "Let's see the ReLU in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb578650",
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = nn.ReLU()\n",
    "\n",
    "x = torch.randn(2, 4)\n",
    "print(x)\n",
    "z = relu(x)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af16a6fd",
   "metadata": {},
   "source": [
    "As you may have noticed, `nn.Module`s are class objects, a bit like scikit-learn models, that you instantiate and then call.\n",
    "\n",
    "You can chain `nn.Module`s with the use of `nn.Sequential`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99cff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_and_relu = nn.Sequential(nn.Linear(20, 40), nn.ReLU())\n",
    "\n",
    "x = torch.randn(16, 20)\n",
    "z = linear_and_relu(x)\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec94fd19",
   "metadata": {},
   "source": [
    "Or even longer constructs:\n",
    "\n",
    "Always keep in mind what happens with the dimensions of your input and outputs with every layer, if your first layer outputs 40 features/nodes/hidden dimensions, then logically the next will have to take 40 as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af81019",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_whole_damn_network = nn.Sequential(\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 16),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(16, 4),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(4, 1)\n",
    "    )\n",
    "\n",
    "x = torch.randn(16, 128)\n",
    "z = a_whole_damn_network(x)\n",
    "print(z.shape)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c237f8f0",
   "metadata": {},
   "source": [
    "The output $z$ that we now obtain after this whole network are called **logits**. They are the real-numbered $\\mathbb{R}$ outputs that we obtain at the end of the network before our last activation function. This last activation function will be a, depending on the task at hand, sigmoid, softmax, or nothing at all for regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be612a49",
   "metadata": {},
   "source": [
    "Similar implementations exist for all types of layers in PyTorch (e.g. Dropout), we invite you to look them up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1564f",
   "metadata": {},
   "source": [
    "### 3.2 Class object neural networks and hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "396393ae",
   "metadata": {},
   "source": [
    "We've seen how to implement a neural network using PyTorch `nn.Sequential`. It is more flexible however to write our own model class. This allows us to have more control over which operations we use and define our own hyperparameters. To make a PyTorch model, we specify our class object to be a submodule of `nn.Module` and inherit its methods via `super().__init__()`. Further, we specify all necessary attributes (such as layers) in our `__init__` function (executed upon initialization) and implement a `forward` function which will be executed when we call the object after being initialized.\n",
    "\n",
    "The following code shows two examples, the first one of a very basic construction of a neural network without hyperparameters. The other one shows the same network, but where we set up our `__init__` function to process hyperparameters as input arguments. We can for example specify a hyperparameter whether we want to use dropout or not. (We could go even further to have an extra hyperparameter specifying the probability of dropout, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2646dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(50, 40)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.layer2 = nn.Linear(40, 20)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.layer3 = nn.Linear(20, 10)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "\n",
    "        self.layer4 = nn.Linear(10, 5)\n",
    "        # Think again: why do we not want a relu and dropout after our last layer again?\n",
    "\n",
    "    def forward(self, x):\n",
    "        # call them in separate lines:\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # or together:\n",
    "        x = self.dropout2(self.relu2(self.layer2(x)))\n",
    "        x = self.dropout3(self.relu3(self.layer3(x)))\n",
    "\n",
    "        # we could've also wrapped everything in a nn.Sequential ..\n",
    "\n",
    "        x = self.layer4(x)\n",
    "        return x\n",
    "\n",
    "class HyperparameterModel(nn.Module):\n",
    "    def __init__(self, dimensions_from_input_to_output = [50, 40, 20, 10, 5], dropout = True):\n",
    "        super().__init__()\n",
    "\n",
    "        layers = []\n",
    "        # iterate through all layers:\n",
    "        for i in range(len(dimensions_from_input_to_output) - 2):\n",
    "            layer = nn.Linear(dimensions_from_input_to_output[i], dimensions_from_input_to_output[i + 1])\n",
    "            layers.append(layer)\n",
    "            layers.append(nn.ReLU())\n",
    "            if dropout == True:\n",
    "                layers.append(nn.Dropout(0.2))\n",
    "\n",
    "\n",
    "        # the last layer separate from the loop because we don't want a ReLU and dropout after the last layer\n",
    "        layer = nn.Linear(dimensions_from_input_to_output[i+1], dimensions_from_input_to_output[i + 2])\n",
    "        layers.append(layer)\n",
    "\n",
    "        # wrap the layers in a sequential\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c847fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BasicModel()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b852d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 50)\n",
    "y = net(x)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b268276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = HyperparameterModel()\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eeda138",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 50)\n",
    "y = net(x)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605fefc7",
   "metadata": {},
   "source": [
    "Or by specifying hyperparameters, the following code shows a bit of a deeper model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a15e373",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = HyperparameterModel(dimensions_from_input_to_output = [50, 160, 80, 40, 20, 10, 5], dropout = False)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194676b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 50)\n",
    "y = net(x)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a55e8d1",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE:</b>\n",
    "<p> Now it is time to create your own model. This model should contain a \"residual block\". <a href=\"https://en.wikipedia.org/wiki/Residual_neural_network\">Residual connections</a>  are a common feature where the data is allowed to skip some of the layers before being added to the output of those layers. We will worry about the reason for such connections later in the course.</p>\n",
    "\n",
    "<img src='https://upload.wikimedia.org/wikipedia/commons/b/ba/ResBlock.png' width=400>\n",
    "\n",
    "Create a model that takes 25 input features, has a residual block with two linear layers of each 25 nodes, before reducing to the number of classes provided as hyperparameter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ef93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## YOUR CODE HERE #########\n",
    "\n",
    "    \n",
    "#################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6764e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the ResidualModel\n",
    "model = ResidualModel(num_classes=5)\n",
    "\n",
    "# Create test input with 25 features\n",
    "test_input = torch.randn(4, 25)\n",
    "print(\"Input shape:\", test_input.shape)\n",
    "\n",
    "# Forward pass\n",
    "output = model(test_input)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"Output values:\", output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55750b8",
   "metadata": {},
   "source": [
    "### 3.3 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bdb83e",
   "metadata": {},
   "source": [
    "We will now take a look at how to load data in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637634c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.random.randn(100, 784)\n",
    "y_train = np.random.randint(low = 0, high = 10, size = (100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train)\n",
    "print(X_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7b181f",
   "metadata": {},
   "source": [
    "Remember to look at your data types: by default NumPy is `float64`, but if you instantiate a model, by default it will have weights in `float32`. It is therefore advised to convert your data to `float32`. In PyTorch, simply `float` is shorthand for `float32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2371bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.float()\n",
    "# Equivalent: X_train.to(torch.float) or X_train.to(torch.float32)\n",
    "print(X_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626e340e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = torch.tensor(y_train)\n",
    "print(y_train.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354e0abf",
   "metadata": {},
   "source": [
    "PyTorch has a very flexible and powerful data loading API. The core objects are the `Dataset` and the `DataLoader`. A `Dataset` is an object that represents your whole dataset, while a `DataLoader` is an object that can iterate through your dataset in batches.\n",
    "\n",
    "Now that we have our X_train and y_train as tensors, we can make them PyTorch-ready by wrapping them in these two objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bebde2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2860dbd8",
   "metadata": {},
   "source": [
    "Now we can use our train_dataloader as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c574734",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    X, y = batch\n",
    "    print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8cb71",
   "metadata": {},
   "source": [
    "As you can see, we can iterate through our batches by use of a for loop, and it will spit out a training batch consisting of a list of X and y tensors. We can also test out code by isolating one training batch like this (only necessary for testing out code):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7f31a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ece9a5",
   "metadata": {},
   "source": [
    "Now we will look at the MNIST dataset, a classical dataset for image classification included in `torchvision`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7cbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "train_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = True,\n",
    "    transform = ToTensor(),\n",
    "    download = True,\n",
    ")\n",
    "test_data = datasets.MNIST(\n",
    "    root = 'data',\n",
    "    train = False,\n",
    "    transform = ToTensor()\n",
    ")\n",
    "\n",
    "X_train = train_data.data\n",
    "y_train = train_data.targets\n",
    "\n",
    "X_test = test_data.data\n",
    "y_test = test_data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d990d517",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('shapes:')\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print('first training image tensor:')\n",
    "print(X_train[0])\n",
    "print('first five labels:')\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d11694",
   "metadata": {},
   "source": [
    "Our data is images, each data sample has $28 \\times 28$ input features, signifying the pixels. We have only seen models that take a 1D vector as input, therefore we need to flatten these features. Later we will see more advanced architectures that can work with 2D images directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e8c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28 * 28)\n",
    "X_test = X_test.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91194bf1",
   "metadata": {},
   "source": [
    "In addition, the grayscale values of our images go from 0 to 255. It is perhaps good practice to min-max standardize these numbers by dividing through 255:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfa5650",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af42098",
   "metadata": {},
   "source": [
    "Finally, let's check our datatypes to see if everything is looking good to go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c0f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.dtype, X_test.dtype, y_train.dtype, y_test.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b649b4c6",
   "metadata": {},
   "source": [
    "Let's split up our training set in a training and validation set and finally wrap our data in a data loader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19016f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "train_indices, val_indices = np.split(np.random.permutation(len(X_train)), [int(len(X_train)*0.8)])\n",
    "X_val = X_train[val_indices]\n",
    "y_val = y_train[val_indices]\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
    "\n",
    "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, pin_memory=True, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c3e53",
   "metadata": {},
   "source": [
    "Let's visualize a random batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ec7b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))\n",
    "X_batch, y_batch = batch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 4, 4\n",
    "for i in range(cols * rows):\n",
    "    img, label = X_batch[i], y_batch[i]\n",
    "    figure.add_subplot(rows, cols, i+1)\n",
    "    plt.title(label.item())\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.reshape(-1, 28, 28).squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2051175b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "<b>EXERCISE:</b>\n",
    "<p>Let us now put all of this together. \n",
    "\n",
    "a) Create a model that classifies MNIST digits. Make sure it has to right number of inputs and outputs. The rest is up to you!\n",
    "\n",
    "b) Create the dataloader and use a batch size of 32. Loop through the training data in batches, and for every batch do a forward pass through your model. (Limit yourself to a fixed number of batches so you don't go through the whole dataset for this exercise.) Next PC lab we will see how to do backpropagation and optimize our model, for now just do the forward pass and print the output shape to see if everything is working as expected.</p>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dd691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## YOUR CODE HERE #########\n",
    "# a) Create a model that classifies MNIST digits\n",
    "\n",
    "\n",
    "# b) Create the training dataLoader and loop through training data in batches (limited to 20 batches)\n",
    "\n",
    "#################################\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
