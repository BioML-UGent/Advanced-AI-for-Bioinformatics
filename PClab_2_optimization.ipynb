{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BioML-UGent/Advanced-AI-for-Bioinformatics/blob/main/PClab_2_optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va08ltgupiFI"
      },
      "source": [
        "# PC lab #2: Optimization for neural networks"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation"
      ],
      "metadata": {
        "id": "o24G4ILv6Wur"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "DPBjC-AI6aTn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "t1cmzGhe6kJK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jT_rF-1cMXbi"
      },
      "source": [
        "We've seen how to implement a neural network using PyTorch:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WCWUPuY_I-nq"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class HyperparameterModel(nn.Module):\n",
        "    def __init__(self, dimensions_from_input_to_output = [50, 40, 20, 10, 5], dropout = True):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        # iterate through all layers:\n",
        "        for i in range(len(dimensions_from_input_to_output) - 2):\n",
        "            layer = nn.Linear(dimensions_from_input_to_output[i], dimensions_from_input_to_output[i + 1])\n",
        "            layers.append(layer)\n",
        "            layers.append(nn.ReLU())\n",
        "            if dropout == True:\n",
        "                layers.append(nn.Dropout(0.2))\n",
        "\n",
        "\n",
        "        # the last layer separate from the loop because we don't want a ReLU and dropout after the last layer\n",
        "        layer = nn.Linear(dimensions_from_input_to_output[i+1], dimensions_from_input_to_output[i + 2])\n",
        "        layers.append(layer)\n",
        "\n",
        "        # wrap the layers in a sequential\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a model tailored for MNIST datasaet."
      ],
      "metadata": {
        "id": "xrHOuTEA5Z7-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WuJm_kzwMXce"
      },
      "outputs": [],
      "source": [
        "model = HyperparameterModel(dimensions_from_input_to_output = [784, 160, 80, 40, 20, 10], dropout = False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8pqMG8vMXci"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifxPbgJjMXdJ"
      },
      "source": [
        "For the first part of this PC lab, we will work with the MNIST dataset included in `torchvision`. (If you're running this code locally, you may have to pip install torchvision)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "as-YG3-_MXdM"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "train_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = True,\n",
        "    transform = ToTensor(),\n",
        "    download = True,\n",
        ")\n",
        "test_data = datasets.MNIST(\n",
        "    root = 'data',\n",
        "    train = False,\n",
        "    transform = ToTensor()\n",
        ")\n",
        "\n",
        "X_train = train_data.data\n",
        "y_train = train_data.targets\n",
        "\n",
        "X_test = test_data.data\n",
        "y_test = test_data.targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NA03h75wMXdV"
      },
      "source": [
        "Our data is images, each data sample has $28 \\times 28$ input features, signifying the pixels. In order to feed this data to our model, we will need to flatten these features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IgNA_Y8gMXdW"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.reshape(-1, 28 * 28)\n",
        "X_test = X_test.reshape(-1, 28 * 28)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQryYAP6MXda"
      },
      "source": [
        "In addition, the grayscale values of our images go from 0 to 255. It is perhaps good practice to min-max standardize these numbers by dividing through 255:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oUXvQOqXMXdb"
      },
      "outputs": [],
      "source": [
        "X_train = X_train / 255\n",
        "X_test = X_test / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3mE5c6nMXdk"
      },
      "source": [
        "Let's split up our training set in a training and validation set and finally wrap our data in a data loader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v9DQFGa3MXdl"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "train_indices, val_indices = np.split(np.random.permutation(len(X_train)), [int(len(X_train)*0.8)])\n",
        "X_val = X_train[val_indices]\n",
        "y_val = y_train[val_indices]\n",
        "X_train = X_train[train_indices]\n",
        "y_train = y_train[train_indices]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "val_dataset = torch.utils.data.TensorDataset(X_val, y_val)\n",
        "val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=16, pin_memory=True, shuffle=True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=16, pin_memory=True, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu4GXH_2MXdn"
      },
      "source": [
        "Let's visualize a random batch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wn3rRoSTMXdq"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "X_batch, y_batch = batch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "figure = plt.figure(figsize=(10, 8))\n",
        "cols, rows = 4, 4\n",
        "for i in range(cols * rows):\n",
        "    img, label = X_batch[i], y_batch[i]\n",
        "    figure.add_subplot(rows, cols, i+1)\n",
        "    plt.title(label.item())\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.reshape(-1, 28, 28).squeeze(), cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGmLYd15MXdw"
      },
      "source": [
        "## 1. Training a neural network\n",
        "\n",
        "The most basic blueprint of PyTorch model training consists of\n",
        "- Get your data\n",
        "- Wrap your data splits in a [data loader](https://pytorch.org/docs/stable/data.html)\n",
        "- Instantiate the model\n",
        "- Instantiate a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "- Instantiate an [optimizer object](https://pytorch.org/docs/stable/optim.html), to which you pass the parameters you want to optimize\n",
        "- Iterate through your training data, for every batch:\n",
        "    - reset the gradients\n",
        "    - do forward pass\n",
        "    - compute loss\n",
        "    - backward pass\n",
        "    - update parameters\n",
        "\n",
        "(Optionally):\n",
        "- After every full iteration through all training data samples (called an epoch), loop through all batches of validation data:\n",
        "    - forward pass\n",
        "    - compute loss and validation scores\n",
        "\n",
        "\n",
        "We already implemented a model compatible with MNIST: 784 input features and 10 output nodes (one for each class). Hence, we can move on to loss function and optimizers. For multi-class classification of the digits, we will use the [Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss). Take a look at what kind of inputs this loss function expects. According to the documentation: `The input is expected to contain raw, unnormalized scores for each class.` Meaning that we can pass logits directly to this loss function, and we do not have to apply a softmax operation ourselves.\n",
        "\n",
        "For optimizer, we can choose [stochastic gradient descent](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD).\n",
        "\n",
        "Take note that we can also specify our desired learning rate in our model. This learning should almost always be tuned as it will influence how fast our model trains but also convergence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "5kZwMU_IMXd1"
      },
      "outputs": [],
      "source": [
        "model = HyperparameterModel(dimensions_from_input_to_output= [784, 128, 10]) # your model from previous exercises here.\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005) # SGD = stochastic gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGrEVraTMXd7"
      },
      "source": [
        "Now we're ready to perform training. We'll build up the training loop step-wise in the following codeblocks. Let's first start with passing one batch to the model, computing the loss, performing backpropagation and updating the weights:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROoh6JGdMXd8"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))\n",
        "X_batch, y_batch = batch\n",
        "\n",
        "y_hat_batch = model(X_batch)\n",
        "\n",
        "loss = loss_function(y_hat_batch, y_batch) # Compute loss\n",
        "\n",
        "loss.backward()   # Calculate gradients\n",
        "optimizer.step()   # Update weights using defined optimizer\n",
        "\n",
        "print(X_batch.shape, y_batch.shape)\n",
        "print(y_hat_batch.shape)\n",
        "print(\"Outputs as logits, first two samples:\")\n",
        "print(y_hat_batch[:2])\n",
        "print('loss:', loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hlu1gnRMXd_"
      },
      "source": [
        "Everytime we perform a training step, we should reset the gradients so that the gradients computed on the previous batch do not influence the next. We can do this by calling `.zero_grad()` on our optimizer. In practice, it is most safe to call this before every forward pass. So if we train a complete epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xiDrim2GMXeE"
      },
      "outputs": [],
      "source": [
        "all_losses = []\n",
        "\n",
        "for batch in train_dataloader:\n",
        "    optimizer.zero_grad()\n",
        "    X_batch, y_batch = batch\n",
        "\n",
        "    y_hat_batch = model(X_batch)\n",
        "\n",
        "    loss = loss_function(y_hat_batch, y_batch) # Compute loss\n",
        "\n",
        "    loss.backward()   # Calculate gradients\n",
        "    optimizer.step()   # Update weights using defined optimizer\n",
        "\n",
        "    all_losses.append(loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXPQQS4QMXeR"
      },
      "source": [
        "Plotting the loss function of every batch during one epoch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgO5nCJJMXeZ"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(len(all_losses)), all_losses)\n",
        "smoothed_losses = np.convolve(all_losses, np.ones(50)/50, mode = \"valid\")\n",
        "plt.plot(np.arange(len(smoothed_losses)), smoothed_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bD36b8QPMXee"
      },
      "source": [
        "We have evaluated the training progress during one epoch on our training set. What if we want to see the performance of the validation set during training, so that we can see if/when the model starts overfitting? It is common practice to perform a pass through the whole validation set after every training epoch. However, we should not compute gradients now, we can block automatic gradient computation using `torch.no_grad()`. Also, we need to tell PyTorch to put our model in evaluation mode (so it does not perform dropout anymore, etc...). After the validation epoch, we should put the model back to training mode again. The code should look something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw0wKUcNMXeh"
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "true_labels = []\n",
        "losses = []\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in val_dataloader:\n",
        "        X_batch, y_batch = batch\n",
        "\n",
        "        y_hat_batch = model(X_batch)\n",
        "\n",
        "        loss = loss_function(y_hat_batch, y_batch)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        predictions.append(y_hat_batch)\n",
        "        true_labels.append(y_batch)\n",
        "\n",
        "model.train()\n",
        "\n",
        "predictions = torch.cat(predictions)\n",
        "true_labels = torch.cat(true_labels)\n",
        "accuracy = (true_labels == predictions.argmax(-1)).sum().item() / len(predictions)\n",
        "\n",
        "print(accuracy)\n",
        "print(np.mean(losses))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f_yo3RiMXeq"
      },
      "source": [
        "#### Exercise 1\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<p> Using the code above, put it all together to train a model for multiple epochs. After every epoch, print or save some training and validation statistics.\n",
        "\n",
        "Monitor how good your model is training. What things could you change? In particular, try tweaking the learning rate. How does this influence training?\n",
        "\n",
        "</p>\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj0vgM2WMXeu",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "N_EPOCHS = 20\n",
        "\n",
        "model =  # your model from previous exercises here.\n",
        "\n",
        "# loss function & optimizer\n",
        "\n",
        "for i in range(1, N_EPOCHS + 1):\n",
        "\n",
        "    # train loop\n",
        "\n",
        "    # eval loop\n",
        "\n",
        "    # record or print some variables that you want to keep track of during training\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04jmvNeBMXe0"
      },
      "source": [
        "#### Exercise 2\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<p> After training, what is your performance on the test dataset?\n",
        "\n",
        "You should be able to obtain an accuracy of +- 96%\n",
        "</p>\n",
        "\n",
        "\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8J8XfWlPMXe2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "######################"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving a model"
      ],
      "metadata": {
        "id": "xVyLFebx6Tgr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After finish training a model, we save the model to disk so that we can load the same weights at a later time. For this, we extract the so-called state_dict from the model which contains all learnable parameters. For our simple model, the state dict contains the following entries:"
      ],
      "metadata": {
        "id": "CCaxpyLW7gVh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWhLoLgA6IEa"
      },
      "outputs": [],
      "source": [
        "state_dict = model.state_dict()\n",
        "print(state_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save the state dictionary, we can use torch.save:"
      ],
      "metadata": {
        "id": "f1b4Gh277iZH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.save(object, filename). For the filename, any extension can be used\n",
        "torch.save(state_dict, \"our_model.tar\")"
      ],
      "metadata": {
        "id": "gaPIqhE47bo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To load a model from a state dict, we use the function torch.load to load the state dict from the disk, and the module function load_state_dict to overwrite our parameters with the new values:"
      ],
      "metadata": {
        "id": "RGflJPds8BD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load state dict from the disk (make sure it is the same name as above)\n",
        "state_dict = torch.load(\"our_model.tar\")"
      ],
      "metadata": {
        "id": "83tQ4YRu8EL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<p> Create a new model, and load the weights of the trained model into new one. Verify this is done properly by printing the weights. </p>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "0HIChyZmunGQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE ###\n",
        "\n",
        "######################"
      ],
      "metadata": {
        "id": "qZhSLM_0uZEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  2. Effect of initialization on training\n"
      ],
      "metadata": {
        "id": "5Gqal5iz6dKn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will review techniques for initialization of neural networks. When increasing the depth of neural networks, there are various challenges we face. Most importantly, we need to have a stable gradient flow through the network, as otherwise, we might encounter vanishing or exploding gradients. This is why we will take a closer look at the effect of initialization on gradients.\n",
        "\n",
        "First, we will review different initialization techniques, and go step by step from the simplest initialization to methods that are nowadays used in very deep networks."
      ],
      "metadata": {
        "id": "hMPMyYjjpmLN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Preparation***\n"
      ],
      "metadata": {
        "id": "IMUPWcCdp08p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Firstly, let’s set up the dataset we want to train it on, namely FashionMNIST. FashionMNIST is a more complex version of MNIST and contains black-and-white images of clothes instead of digits. The 10 classes include trousers, coats, shoes, bags and more. To load this dataset, we will make use of torchvision. The torchvision package consists of popular datasets, model architectures, and common image transformations for computer vision. We will use the package for many of the notebooks in this course to simplify our dataset handling.\n",
        "\n",
        "Let’s load the dataset below, and visualize a few images to get an impression of the data."
      ],
      "metadata": {
        "id": "0OF41HP4ka8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.datasets import FashionMNIST\n",
        "from torchvision import transforms\n",
        "import torch.utils.data as data\n",
        "DATASET_PATH = \"../data\"\n",
        "# Transformations applied on each image => first make them a tensor, then normalize them in the range -1 to 1\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Loading the training dataset. We need to split it into a training and validation part\n",
        "train_dataset = FashionMNIST(root=DATASET_PATH, train=True, transform=transform, download=True)\n",
        "train_set, val_set = torch.utils.data.random_split(train_dataset, [50000, 10000])\n",
        "\n",
        "# Loading the test set\n",
        "test_set = FashionMNIST(root=DATASET_PATH, train=False, transform=transform, download=True)\n",
        "\n",
        "# We define a set of data loaders that we can use for various purposes later.\n",
        "# Note that for actually training a model, we will use different data loaders\n",
        "# with a lower batch size.\n",
        "train_loader = data.DataLoader(train_set, batch_size=1024, shuffle=True, drop_last=False)\n",
        "val_loader = data.DataLoader(val_set, batch_size=1024, shuffle=False, drop_last=False)\n",
        "test_loader = data.DataLoader(test_set, batch_size=1024, shuffle=False, drop_last=False)"
      ],
      "metadata": {
        "id": "v6OSpaI4khrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exmp_imgs = [train_set[i][0] for i in range(16)]\n",
        "# Organize the images into a grid for nicer visualization\n",
        "img_grid = torchvision.utils.make_grid(torch.stack(exmp_imgs, dim=0), nrow=4, normalize=True, pad_value=0.5)\n",
        "img_grid = img_grid.permute(1, 2, 0)\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.title(\"FashionMNIST examples\")\n",
        "plt.imshow(img_grid)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "veLPBaZ_kj9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<p> Set up a neural network. The chosen network should view the images as 1D tensors and pushes them through a sequence of linear layers and a specified activation function. </p>\n",
        "\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "-chuT74t2IGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, act_fn, input_size=784, num_classes=10, hidden_sizes=[512, 256, 256, 128]):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            act_fn - Object of the activation function that should be used as non-linearity in the network.\n",
        "            input_size - Size of the input images in pixels\n",
        "            num_classes - Number of classes we want to predict\n",
        "            hidden_sizes - A list of integers specifying the hidden layer sizes in the NN\n",
        "        \"\"\"\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        ######################\n",
        "\n",
        "        self.config = {\"act_fn\": act_fn.__class__.__name__, \"input_size\": input_size, \"num_classes\": num_classes, \"hidden_sizes\": hidden_sizes}\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        ### YOUR CODE HERE ###\n",
        "\n",
        "        ######################\n",
        "\n",
        "        return ..."
      ],
      "metadata": {
        "id": "RnSdEWhVqhLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the activation functions, we make use of PyTorch’s torch.nn library instead of implementing ourselves. However, we also define an Identity activation function. Although this activation function would significantly limit the network’s modeling capabilities, we will use it in the first steps of our discussion about initialization (for simplicity).\n",
        "\n"
      ],
      "metadata": {
        "id": "YSTdRf8DqjAB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Identity(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x\n",
        "\n",
        "act_fn_by_name = {\n",
        "    \"tanh\": nn.Tanh,\n",
        "    \"relu\": nn.ReLU,\n",
        "    \"identity\": Identity\n",
        "}"
      ],
      "metadata": {
        "id": "5BAZPLjpqk0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we define a few plotting functions that we will use for our discussions. These functions help us to (1) visualize the weight/parameter distribution inside a network, (2) visualize the gradients that the parameters at different layers receive, and (3) the activations, i.e. the output of the linear layers. The detailed code is not important, but feel free to take a closer look if interested."
      ],
      "metadata": {
        "id": "vzgpKIyBqmmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "import seaborn as sns\n",
        "import math\n",
        "##############################################################\n",
        "\n",
        "def plot_dists(val_dict, color=\"C0\", xlabel=None, stat=\"count\", use_kde=True):\n",
        "    columns = len(val_dict)\n",
        "    fig, ax = plt.subplots(1, columns, figsize=(columns*3, 2.5))\n",
        "    fig_index = 0\n",
        "    for key in sorted(val_dict.keys()):\n",
        "        key_ax = ax[fig_index%columns]\n",
        "        sns.histplot(val_dict[key], ax=key_ax, color=color, bins=50, stat=stat,\n",
        "                     kde=use_kde and ((val_dict[key].max()-val_dict[key].min())>1e-8)) # Only plot kde if there is variance\n",
        "        key_ax.set_title(f\"{key} \" + (r\"(%i $\\to$ %i)\" % (val_dict[key].shape[1], val_dict[key].shape[0]) if len(val_dict[key].shape)>1 else \"\"))\n",
        "        if xlabel is not None:\n",
        "            key_ax.set_xlabel(xlabel)\n",
        "        fig_index += 1\n",
        "    fig.subplots_adjust(wspace=0.4)\n",
        "    return fig\n",
        "\n",
        "##############################################################\n",
        "\n",
        "def visualize_weight_distribution(model, color=\"C0\"):\n",
        "    weights = {}\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.endswith(\".bias\"):\n",
        "            continue\n",
        "        key_name = f\"Layer {name.split('.')[1]}\"\n",
        "        weights[key_name] = param.detach().view(-1).cpu().numpy()\n",
        "\n",
        "    ## Plotting\n",
        "    fig = plot_dists(weights, color=color, xlabel=\"Weight vals\")\n",
        "    fig.suptitle(\"Weight distribution\", fontsize=14, y=1.05)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "##############################################################\n",
        "\n",
        "def visualize_gradients(model, color=\"C0\", print_variance=False):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        net - Object of class BaseNetwork\n",
        "        color - Color in which we want to visualize the histogram (for easier separation of activation functions)\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n",
        "    imgs, labels = next(iter(small_loader))\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "    # Pass one batch through the network, and calculate the gradients for the weights\n",
        "    model.zero_grad()\n",
        "    preds = model(imgs)\n",
        "    loss = F.cross_entropy(preds, labels) # Same as nn.CrossEntropyLoss, but as a function instead of module\n",
        "    loss.backward()\n",
        "    # We limit our visualization to the weight parameters and exclude the bias to reduce the number of plots\n",
        "    grads = {name: params.grad.view(-1).cpu().clone().numpy() for name, params in model.named_parameters() if \"weight\" in name}\n",
        "    model.zero_grad()\n",
        "\n",
        "    ## Plotting\n",
        "    fig = plot_dists(grads, color=color, xlabel=\"Grad magnitude\")\n",
        "    fig.suptitle(\"Gradient distribution\", fontsize=14, y=1.05)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    if print_variance:\n",
        "        for key in sorted(grads.keys()):\n",
        "            print(f\"{key} - Variance: {np.var(grads[key])}\")\n",
        "\n",
        "##############################################################\n",
        "\n",
        "def visualize_activations(model, color=\"C0\", print_variance=False):\n",
        "    model.eval()\n",
        "    small_loader = data.DataLoader(train_set, batch_size=1024, shuffle=False)\n",
        "    imgs, labels = next(iter(small_loader))\n",
        "    imgs, labels = imgs.to(device), labels.to(device)\n",
        "\n",
        "    # Pass one batch through the network, and calculate the gradients for the weights\n",
        "    feats = imgs.view(imgs.shape[0], -1)\n",
        "    activations = {}\n",
        "    with torch.no_grad():\n",
        "        for layer_index, layer in enumerate(model.layers):\n",
        "            feats = layer(feats)\n",
        "            if isinstance(layer, nn.Linear):\n",
        "                activations[f\"Layer {layer_index}\"] = feats.view(-1).detach().cpu().numpy()\n",
        "\n",
        "    ## Plotting\n",
        "    fig = plot_dists(activations, color=color, stat=\"density\", xlabel=\"Activation vals\")\n",
        "    fig.suptitle(\"Activation distribution\", fontsize=14, y=1.05)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    if print_variance:\n",
        "        for key in sorted(activations.keys()):\n",
        "            print(f\"{key} - Variance: {np.var(activations[key])}\")\n",
        "\n",
        "\n",
        "##############################################################"
      ],
      "metadata": {
        "id": "olZT12HPqoSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When initializing a neural network, there are a few properties we would like to have.\n",
        "\n",
        "1- First, the variance of the input should be propagated through the model to the last layer, so that we have a similar standard deviation for the output neurons. If the variance would vanish the deeper we go in our model, it becomes much harder to optimize the model as the input to the next layer is basically a single constant value. Similarly, if the variance increases, it is likely to explode (i.e. head to infinity) the deeper we design our model.\n",
        "\n",
        "2- The second property we look out for in initialization techniques is a gradient distribution with equal variance across layers. If the first layer receives much smaller gradients than the last layer, we will have difficulties in choosing an appropriate learning rate.\n",
        "\n",
        "As a starting point for finding a good method, we will analyze different initialization based on our linear neural network with no activation function (i.e. an identity). We do this because initializations depend on the specific activation function used in the network, and we can adjust the initialization schemes later on for our specific choice.\n",
        "\n"
      ],
      "metadata": {
        "id": "KzhUN1BfqrBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\") if not torch.cuda.is_available() else torch.device(\"cuda:0\")\n",
        "model = BaseNetwork(act_fn=Identity()).to(device)"
      ],
      "metadata": {
        "id": "Yx2pfZAoqtrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Constant initialization\n",
        "\n",
        "The first initialization we can consider is to initialize all weights with the same constant value. Intuitively, setting all weights to zero is not a good idea as the propagated gradient will be zero. However, what happens if we set all weights to a value slightly larger or smaller than 0? To find out, we can implement a function for setting all parameters below and visualize the gradients.\n",
        "\n"
      ],
      "metadata": {
        "id": "KuDRf8OFqxHA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def const_init(model, c=0.0):\n",
        "    for name, param in model.named_parameters():\n",
        "        param.data.fill_(c)\n",
        "\n",
        "const_init(model, c=0.005)\n",
        "visualize_gradients(model)\n",
        "visualize_activations(model, print_variance=True)"
      ],
      "metadata": {
        "id": "2cwUXJnkqzDB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, only the first and the last layer have diverse gradient distributions while the other three layers have the same gradient for all weights (note that this value is unequal 0, but often very close to it). Having the same gradient for parameters that have been initialized with the same values means that we will always have the same value for those parameters. This would make our layer useless and reduce our effective number of parameters to 1. Thus, we cannot use a constant initialization to train our networks.\n"
      ],
      "metadata": {
        "id": "h1-uOmEOq3P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Constant variance\n",
        "\n",
        "From the experiment above, we have seen that a constant value is not working. So instead, how about we initialize the parameters by randomly sampling from a distribution like a Gaussian? The most intuitive way would be to choose one variance that is used for all layers in the network."
      ],
      "metadata": {
        "id": "hBoZormK46bZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<p> Implement random initialization (with constant variance) below, and visualize the gradients and activation distribution across layers. </p>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "FXRliICs5pE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def var_init(model, std=0.01):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    ######################\n"
      ],
      "metadata": {
        "id": "FO8JJGTkq6ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The variance of the activation becomes smaller and smaller across layers, and almost vanishes in the last layer. Alternatively, we could use a higher standard deviation:"
      ],
      "metadata": {
        "id": "boRzx9ajq9Wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var_init(model, std=0.1)\n",
        "visualize_gradients(model)\n",
        "visualize_activations(model, print_variance=True)\n"
      ],
      "metadata": {
        "id": "AnN4t7qDq_Py"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmIK8skGrFcm"
      },
      "source": [
        "### How to find appropriate initialization values\n",
        "\n",
        "From our experiments above, we have seen that we need to sample the weights from a distribution, but are not sure which one exactly. As a next step, we will try to find the optimal initialization from the perspective of the activation distribution. For this, we state two requirements:\n",
        "\n",
        "1. The mean of the activations should be zero\n",
        "2. The variance of the activations should stay the same across every layer\n",
        "\n",
        "Suppose we want to design an initialization for the following layer:\n",
        "\n",
        "$y=Wx+b$ with $y\\in\\mathbb{R}^{d_y}$, $x\\in\\mathbb{R}^{d_x}$.\n",
        "\n",
        "Our goal is that the variance of each element of $y$ is the same as the input, i.e. $\\text{Var}(y_i)=\\text{Var}(x_i)=\\sigma_x^{2}$, and that the mean is zero. We assume $x$ to also have a mean of zero, because, in deep neural networks, $y$ would be the input of another layer. This requires the bias and weight to have an expectation of 0. Actually, as $b$ is a single element per output neuron and is constant across different inputs, we set it to 0 overall.\n",
        "\n",
        "Next, we need to calculate the variance with which we need to initialize the weight parameters.\n",
        "Along the calculation, we will need the following variance rule: given two independent variables, the variance of their product is $\\text{Var}(X\\cdot Y) = \\mathbb{E}(Y)^2\\text{Var}(X) + \\mathbb{E}(X)^2\\text{Var}(Y) + \\text{Var}(X)\\text{Var}(Y) = \\mathbb{E}(Y^2)\\mathbb{E}(X^2)-\\mathbb{E}(Y)^2\\mathbb{E}(X)^2$ ($X$ and $Y$ are not refering to $x$ and $y$, but any random variable).\n",
        "\n",
        "The needed variance of the weights, $\\text{Var}(w_{ij})$, is calculated as follows:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    y_i & = \\sum_{j} w_{ij}x_{j}\\hspace{10mm}\\text{Calculation of a single output neuron without bias}\\\\\n",
        "    \\text{Var}(y_i) = \\sigma_x^{2} & = \\text{Var}\\left(\\sum_{j} w_{ij}x_{j}\\right)\\\\\n",
        "    & = \\sum_{j} \\text{Var}(w_{ij}x_{j}) \\hspace{10mm}\\text{Inputs and weights are independent of each other}\\\\\n",
        "    & = \\sum_{j} \\text{Var}(w_{ij})\\cdot\\text{Var}(x_{j}) \\hspace{10mm}\\text{Variance rule (see above) with expectations being zero}\\\\\n",
        "    & = d_x \\cdot \\text{Var}(w_{ij})\\cdot\\text{Var}(x_{j}) \\hspace{10mm}\\text{Variance equal for all $d_x$ elements}\\\\\n",
        "    & = \\sigma_x^{2} \\cdot d_x \\cdot \\text{Var}(w_{ij})\\\\\n",
        "    \\Rightarrow \\text{Var}(w_{ij}) = \\sigma_{W}^2 & = \\frac{1}{d_x}\\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Thus, we should initialize the weight distribution with a variance of the inverse of the input dimension $d_x$."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 6\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<p> Implement the idea above and visualize the gradient and activation distributions: </p>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "wSFsiB_z5vgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def equal_var_init(model):\n",
        "   ### YOUR CODE HERE ###\n",
        "\n",
        "   ######################\n"
      ],
      "metadata": {
        "id": "YjX4cDwRrS0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJPfinotrFcm"
      },
      "source": [
        "As we expected, the variance stays indeed constant across layers. Note that our initialization does not restrict us to a normal distribution, but allows any other distribution with a mean of 0 and variance of $1/d_x$. You often see that a uniform distribution is used for initialization. A small benefit of using a uniform instead of a normal distribution is that we can exclude the chance of initializing very large or small weights.\n",
        "\n",
        "Besides the variance of the activations, another variance we would like to stabilize is the one of the gradients. This ensures a stable optimization for deep networks. It turns out that we can do the same calculation as above starting from $\\Delta x=W\\Delta y$, and come to the conclusion that we should initialize our layers with $1/d_y$ where $d_y$ is the number of output neurons. You can do the calculation as a practice, or check a thorough explanation in [this blog post](https://pouannes.github.io/blog/initialization/#mjx-eqn-eqfwd_K). As a compromise between both constraints, [Glorot and Bengio (2010)](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf?hc_location=ufi) proposed to use the harmonic mean of both values. This leads us to the well-known Xavier initialization:\n",
        "\n",
        "$$W\\sim N\\left(0,\\frac{2}{d_x+d_y}\\right)$$\n",
        "\n",
        "If we use a uniform distribution, we would initialize the weights with:\n",
        "\n",
        "$$W\\sim U\\left[-\\frac{\\sqrt{6}}{\\sqrt{d_x+d_y}}, \\frac{\\sqrt{6}}{\\sqrt{d_x+d_y}}\\right]$$"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 7\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<p>\n",
        "Let's shortly implement it and visualise gradients and activations: </p>\n",
        "\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "p-N0jxme53WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_init(model):\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    ######################"
      ],
      "metadata": {
        "id": "9c_yLM3Nrc3C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the Xavier initialization balances the variance of gradients and activations. Note that the significantly higher variance for the output layer is due to the large difference of input and output dimension ( 128  vs  10 ).\n",
        "\n",
        "However, we currently assumed the activation function to be linear. So what happens if we add a non-linearity? Let's try a tanh-based network:"
      ],
      "metadata": {
        "id": "lm83v6VerfJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BaseNetwork(act_fn=nn.Tanh()).to(device)\n",
        "xavier_init(model)\n",
        "visualize_gradients(model)\n",
        "visualize_activations(model, print_variance=True)"
      ],
      "metadata": {
        "id": "sqRFSapIrfs7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_hJ3qBLrFcm"
      },
      "source": [
        "Although the variance decreases over depth, it is apparent that the activation distribution becomes more focused on the low values. Therefore, our variance will stabilize around 0.25 if we would go even deeper. Hence, we can conclude that the Xavier initialization works well for Tanh networks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAIpr4k8rFcq"
      },
      "source": [
        "## 4. Optimization algorithms\n",
        "\n",
        "Besides initialization, selecting a suitable optimization algorithm can be an important choice for deep neural networks. Before taking a closer look at them, we should define code for training the models. Most of the following code is copied from the previous sections, and only slightly altered to fit our needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQdsRLb5rFcq"
      },
      "outputs": [],
      "source": [
        "def _get_config_file(model_path, model_name):\n",
        "    return os.path.join(model_path, model_name + \".config\")\n",
        "\n",
        "def _get_model_file(model_path, model_name):\n",
        "    return os.path.join(model_path, model_name + \".tar\")\n",
        "\n",
        "def _get_result_file(model_path, model_name):\n",
        "    return os.path.join(model_path, model_name + \"_results.json\")\n",
        "\n",
        "def load_model(model_path, model_name, net=None):\n",
        "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
        "    assert os.path.isfile(config_file), f\"Could not find the config file \\\"{config_file}\\\". Are you sure this is the correct path and you have your model config stored here?\"\n",
        "    assert os.path.isfile(model_file), f\"Could not find the model file \\\"{model_file}\\\". Are you sure this is the correct path and you have your model stored here?\"\n",
        "    with open(config_file, \"r\") as f:\n",
        "        config_dict = json.load(f)\n",
        "    if net is None:\n",
        "        act_fn_name = config_dict[\"act_fn\"].pop(\"name\").lower()\n",
        "        assert act_fn_name in act_fn_by_name, f\"Unknown activation function \\\"{act_fn_name}\\\". Please add it to the \\\"act_fn_by_name\\\" dict.\"\n",
        "        act_fn = act_fn_by_name[act_fn_name]()\n",
        "        net = BaseNetwork(act_fn=act_fn, **config_dict)\n",
        "    net.load_state_dict(torch.load(model_file))\n",
        "    return net\n",
        "\n",
        "def save_model(model, model_path, model_name):\n",
        "    config_dict = model.config\n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    config_file, model_file = _get_config_file(model_path, model_name), _get_model_file(model_path, model_name)\n",
        "    with open(config_file, \"w\") as f:\n",
        "        json.dump(config_dict, f)\n",
        "    torch.save(model.state_dict(), model_file)\n",
        "\n",
        "def train_model(net, model_name, optim_func, max_epochs=50, batch_size=256, overwrite=False):\n",
        "    \"\"\"\n",
        "    Train a model on the training set of FashionMNIST\n",
        "\n",
        "    Inputs:\n",
        "        net - Object of BaseNetwork\n",
        "        model_name - (str) Name of the model, used for creating the checkpoint names\n",
        "        max_epochs - Number of epochs we want to (maximally) train for\n",
        "        patience - If the performance on the validation set has not improved for #patience epochs, we stop training early\n",
        "        batch_size - Size of batches used in training\n",
        "        overwrite - Determines how to handle the case when there already exists a checkpoint. If True, it will be overwritten. Otherwise, we skip training.\n",
        "    \"\"\"\n",
        "    file_exists = os.path.isfile(_get_model_file(CHECKPOINT_PATH, model_name))\n",
        "    if file_exists and not overwrite:\n",
        "        print(f\"Model file of \\\"{model_name}\\\" already exists. Skipping training...\")\n",
        "        with open(_get_result_file(CHECKPOINT_PATH, model_name), \"r\") as f:\n",
        "            results = json.load(f)\n",
        "    else:\n",
        "        if file_exists:\n",
        "            print(\"Model file exists, but will be overwritten...\")\n",
        "\n",
        "        # Defining optimizer, loss and data loader\n",
        "        optimizer =  optim_func(net.parameters())\n",
        "        loss_module = nn.CrossEntropyLoss()\n",
        "        train_loader_local = data.DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True, pin_memory=True)\n",
        "\n",
        "        results = None\n",
        "        val_scores = []\n",
        "        train_losses, train_scores = [], []\n",
        "        best_val_epoch = -1\n",
        "        for epoch in range(max_epochs):\n",
        "            ############\n",
        "            # Training #\n",
        "            ############\n",
        "            net.train()\n",
        "            true_preds, count = 0., 0\n",
        "            t = tqdm(train_loader_local, leave=False)\n",
        "            for imgs, labels in t:\n",
        "                imgs, labels = imgs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                preds = net(imgs)\n",
        "                loss = loss_module(preds, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                # Record statistics during training\n",
        "                true_preds += (preds.argmax(dim=-1) == labels).sum().item()\n",
        "                count += labels.shape[0]\n",
        "                t.set_description(f\"Epoch {epoch+1}: loss={loss.item():4.2f}\")\n",
        "                train_losses.append(loss.item())\n",
        "            train_acc = true_preds / count\n",
        "            train_scores.append(train_acc)\n",
        "\n",
        "            ##############\n",
        "            # Validation #\n",
        "            ##############\n",
        "            val_acc = test_model(net, val_loader)\n",
        "            val_scores.append(val_acc)\n",
        "            print(f\"[Epoch {epoch+1:2d}] Training accuracy: {train_acc*100.0:05.2f}%, Validation accuracy: {val_acc*100.0:05.2f}%\")\n",
        "\n",
        "            if len(val_scores) == 1 or val_acc > val_scores[best_val_epoch]:\n",
        "                print(\"\\t   (New best performance, saving model...)\")\n",
        "                save_model(net, CHECKPOINT_PATH, model_name)\n",
        "                best_val_epoch = epoch\n",
        "\n",
        "    if results is None:\n",
        "        load_model(CHECKPOINT_PATH, model_name, net=net)\n",
        "        test_acc = test_model(net, test_loader)\n",
        "        results = {\"test_acc\": test_acc, \"val_scores\": val_scores, \"train_losses\": train_losses, \"train_scores\": train_scores}\n",
        "        with open(_get_result_file(CHECKPOINT_PATH, model_name), \"w\") as f:\n",
        "            json.dump(results, f)\n",
        "\n",
        "    # Plot a curve of the validation accuracy\n",
        "    sns.set()\n",
        "    plt.plot([i for i in range(1,len(results[\"train_scores\"])+1)], results[\"train_scores\"], label=\"Train\")\n",
        "    plt.plot([i for i in range(1,len(results[\"val_scores\"])+1)], results[\"val_scores\"], label=\"Val\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Validation accuracy\")\n",
        "    plt.ylim(min(results[\"val_scores\"]), max(results[\"train_scores\"])*1.01)\n",
        "    plt.title(f\"Validation performance of {model_name}\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    print((f\" Test accuracy: {results['test_acc']*100.0:4.2f}% \").center(50, \"=\")+\"\\n\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def test_model(net, data_loader):\n",
        "    \"\"\"\n",
        "    Test a model on a specified dataset.\n",
        "\n",
        "    Inputs:\n",
        "        net - Trained model of type BaseNetwork\n",
        "        data_loader - DataLoader object of the dataset to test on (validation or test)\n",
        "    \"\"\"\n",
        "    net.eval()\n",
        "    true_preds, count = 0., 0\n",
        "    for imgs, labels in data_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            preds = net(imgs).argmax(dim=-1)\n",
        "            true_preds += (preds == labels).sum().item()\n",
        "            count += labels.shape[0]\n",
        "    test_acc = true_preds / count\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDtcdDKprFcr"
      },
      "source": [
        "First, we need to understand what an optimizer actually does. The optimizer is responsible to update the network's parameters given the gradients. Hence, we effectively implement a function $w^{t} = f(w^{t-1}, g^{t}, ...)$ with $w$ being the parameters, and $g^{t} = \\nabla_{w^{(t-1)}} L^{(t)}$ the gradients at time step $t$. A common, additional parameter to this function is the learning rate, here denoted by $\\eta$. Usually, the learning rate can be seen as the \"step size\" of the update. A higher learning rate means that we change the weights more in the direction of the gradients, a smaller means we take shorter steps.\n",
        "\n",
        "As most optimizers only differ in the implementation of $f$, we can define a template for an optimizer in PyTorch below. We take as input the parameters of a model and a learning rate. The function `zero_grad` sets the gradients of all parameters to zero, which we have to do before calling `loss.backward()`. Finally, the `step()` function tells the optimizer to update all weights based on their gradients. The template is setup below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaV--DtkrFcr"
      },
      "outputs": [],
      "source": [
        "class OptimizerTemplate:\n",
        "\n",
        "    def __init__(self, params, lr):\n",
        "        self.params = list(params)\n",
        "        self.lr = lr\n",
        "\n",
        "    def zero_grad(self):\n",
        "        ## Set gradients of all parameters to zero\n",
        "        for p in self.params:\n",
        "            if p.grad is not None:\n",
        "                p.grad.detach_() # For second-order optimizers important\n",
        "                p.grad.zero_()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self):\n",
        "        ## Apply update step to all parameters\n",
        "        for p in self.params:\n",
        "            if p.grad is None: # We skip parameters without any gradients\n",
        "                continue\n",
        "            self.update_param(p)\n",
        "\n",
        "    def update_param(self, p):\n",
        "        # To be implemented in optimizer-specific classes\n",
        "        raise NotImplementedError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mi78j4o9rFcr"
      },
      "source": [
        "The first optimizer we are going to implement is the standard Stochastic Gradient Descent (SGD). SGD updates the parameters using the following equation:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    w^{(t)} & = w^{(t-1)} - \\eta \\cdot g^{(t)}\n",
        "\\end{split}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 8\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<p>\n",
        "Implement SGD: </p>\n",
        "\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "TPt-H11_61Gp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWe6Kj2-rFcr"
      },
      "outputs": [],
      "source": [
        "class SGD(OptimizerTemplate):\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    ######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NuPomcHorFcr"
      },
      "source": [
        "In the lecture, we also have discussed the concept of momentum which replaces the gradient in the update by an exponential average of all past gradients including the current one:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    m^{(t)} & = \\beta_1 m^{(t-1)} + (1 - \\beta_1)\\cdot g^{(t)}\\\\\n",
        "    w^{(t)} & = w^{(t-1)} - \\eta \\cdot m^{(t)}\\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Let's also implement it below:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 9\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "\n",
        "<p>\n",
        "Implement SGD with momentum: </p>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "tfeeJtKQ68Ai"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8Vn5hgKrFcr"
      },
      "outputs": [],
      "source": [
        "class SGDMomentum(OptimizerTemplate):\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    ######################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bN_GbsDorFcr"
      },
      "source": [
        "Finally, we arrive at Adam. Adam combines the idea of momentum with an adaptive learning rate, which is based on an exponential average of the squared gradients, i.e. the gradients norm. Furthermore, we add a correction for the momentum and adaptive learning rate:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "    m^{(t)} & = \\beta_1 m^{(t-1)} + (1 - \\beta_1)\\cdot g^{(t)}\\\\\n",
        "    v^{(t)} & = \\beta_2 v^{(t-1)} + (1 - \\beta_2)\\cdot \\left(g^{(t)}\\right)^2\\\\\n",
        "    \\hat{m}^{(t)} & = \\frac{m^{(t)}}{1-\\beta^{t}_1}, \\hat{v}^{(t)} = \\frac{v^{(t)}}{1-\\beta^{t}_2}\\\\\n",
        "    w^{(t)} & = w^{(t-1)} - \\frac{\\eta}{\\sqrt{\\hat{v}^{(t)}} + \\epsilon}\\circ \\hat{m}^{(t)}\\\\\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "Epsilon is a small constant used to improve numerical stability for very small gradient norms. Remember that the adaptive learning rate does not replace the learning rate hyperparameter $\\eta$, but rather acts as an extra factor and ensures that the gradients of various parameters have a similar norm."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 10\n",
        "\n",
        "<div class=\"alert alert-success\">\n",
        "<p>\n",
        "Implement Adam optimizer: </p>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "id": "G-s1yz8B7CZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIAlfmBSrFcr"
      },
      "outputs": [],
      "source": [
        "class Adam(OptimizerTemplate):\n",
        "\n",
        "    ### YOUR CODE HERE ###\n",
        "\n",
        "    ######################\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JESywpsQrFcr"
      },
      "source": [
        "### Pathological curvatures\n",
        "\n",
        "A pathological curvature is a type of surface that is similar to ravines and is particularly tricky for plain SGD optimization. In words, pathological curvatures typically have a steep gradient in one direction with an optimum at the center, while in a second direction we have a slower gradient towards a (global) optimum. Let's first create an example surface of this and visualize it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w17fIQDhrFcr"
      },
      "outputs": [],
      "source": [
        "def pathological_curve_loss(w1, w2):\n",
        "    # Example of a pathological curvature. There are many more possible, feel free to experiment here!\n",
        "    x1_loss = torch.tanh(w1)**2 + 0.01 * torch.abs(w1)\n",
        "    x2_loss = torch.sigmoid(w2)\n",
        "    return x1_loss + x2_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzUUoV1grFcr"
      },
      "outputs": [],
      "source": [
        "from matplotlib import cm\n",
        "\n",
        "def plot_curve(curve_fn, x_range=(-5,5), y_range=(-5,5), plot_3d=False, cmap=cm.viridis, title=\"Pathological curvature\"):\n",
        "    fig = plt.figure()\n",
        "    ax = plt.axes(projection='3d') if plot_3d else plt.axes()\n",
        "\n",
        "    x = torch.arange(x_range[0], x_range[1], (x_range[1]-x_range[0])/100.)\n",
        "    y = torch.arange(y_range[0], y_range[1], (y_range[1]-y_range[0])/100.)\n",
        "    x, y = torch.meshgrid(x, y, indexing='xy')\n",
        "    z = curve_fn(x, y)\n",
        "    x, y, z = x.numpy(), y.numpy(), z.numpy()\n",
        "\n",
        "    if plot_3d:\n",
        "        ax.plot_surface(x, y, z, cmap=cmap, linewidth=1, color=\"#000\", antialiased=False)\n",
        "        ax.set_zlabel(\"loss\")\n",
        "    else:\n",
        "        ax.imshow(z[::-1], cmap=cmap, extent=(x_range[0], x_range[1], y_range[0], y_range[1]))\n",
        "    plt.title(title)\n",
        "    ax.set_xlabel(r\"$w_1$\")\n",
        "    ax.set_ylabel(r\"$w_2$\")\n",
        "    plt.tight_layout()\n",
        "    return ax\n",
        "\n",
        "sns.reset_orig()\n",
        "_ = plot_curve(pathological_curve_loss, plot_3d=True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQH-owNvrFcr"
      },
      "source": [
        "In terms of optimization, you can image that $w_1$ and $w_2$ are weight parameters, and the curvature represents the loss surface over the space of $w_1$ and $w_2$. Note that in typical networks, we have many, many more parameters than two, and such curvatures can occur in multi-dimensional spaces as well.\n",
        "\n",
        "Ideally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of $w_2$. However, if we encounter a point along the ridges, the gradient is much greater in $w_1$ than $w_2$, and we might end up jumping from one side to the other. Due to the large gradients, we would have to reduce our learning rate slowing down learning significantly.\n",
        "\n",
        "To test our algorithms, we can implement a simple function to train two parameters on such a surface:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyyhUyE5rFcs"
      },
      "outputs": [],
      "source": [
        "def train_curve(optimizer_func, curve_func=pathological_curve_loss, num_updates=100, init=[5,5]):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "        optimizer_func - Constructor of the optimizer to use. Should only take a parameter list\n",
        "        curve_func - Loss function (e.g. pathological curvature)\n",
        "        num_updates - Number of updates/steps to take when optimizing\n",
        "        init - Initial values of parameters. Must be a list/tuple with two elements representing w_1 and w_2\n",
        "    Outputs:\n",
        "        Numpy array of shape [num_updates, 3] with [t,:2] being the parameter values at step t, and [t,2] the loss at t.\n",
        "    \"\"\"\n",
        "    weights = nn.Parameter(torch.FloatTensor(init), requires_grad=True)\n",
        "    optimizer = optimizer_func([weights])\n",
        "\n",
        "    list_points = []\n",
        "    for _ in range(num_updates):\n",
        "        loss = curve_func(weights[0], weights[1])\n",
        "        list_points.append(torch.cat([weights.data.detach(), loss.unsqueeze(dim=0).detach()], dim=0))\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    points = torch.stack(list_points, dim=0).numpy()\n",
        "    return points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcv7K9wsrFcs"
      },
      "source": [
        "Next, let's apply the different optimizers on our curvature. Note that we set a much higher learning rate for the optimization algorithms as you would in a standard neural network. This is because we only have 2 parameters instead of tens of thousands or even millions."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WBwl66VLNR7c"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8phNFzzErFcs"
      },
      "outputs": [],
      "source": [
        "SGD_points = train_curve(lambda params: SGD(params, lr=10))\n",
        "#SGDMom_points = train_curve(lambda params: SGDMomentum(params, lr=10, momentum=0.9))\n",
        "#Adam_points = train_curve(lambda params: Adam(params, lr=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgrl572ArFcs"
      },
      "source": [
        "To understand best how the different algorithms worked, we visualize the update step as a line plot through the loss surface. We will stick with a 2D representation for readability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9rPzgZ8rFcs"
      },
      "outputs": [],
      "source": [
        "all_points = np.concatenate([SGD_points], axis=0)\n",
        "#all_points = np.concatenate([SGD_points, SGDMom_points], axis=0)\n",
        "#all_points = np.concatenate([SGD_points, SGDMom_points, Adam_points], axis=0)\n",
        "\n",
        "ax = plot_curve(pathological_curve_loss,\n",
        "                x_range=(-np.absolute(all_points[:,0]).max(), np.absolute(all_points[:,0]).max()),\n",
        "                y_range=(all_points[:,1].min(), all_points[:,1].max()),\n",
        "                plot_3d=False)\n",
        "ax.plot(SGD_points[:,0], SGD_points[:,1], color=\"red\", marker=\"o\", zorder=1, label=\"SGD\")\n",
        "#ax.plot(SGDMom_points[:,0], SGDMom_points[:,1], color=\"blue\", marker=\"o\", zorder=2, label=\"SGDMom\")\n",
        "#ax.plot(Adam_points[:,0], Adam_points[:,1], color=\"grey\", marker=\"o\", zorder=3, label=\"Adam\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31cbkuSMrFcs"
      },
      "source": [
        "We can clearly see that SGD is not able to find the center of the optimization curve and has a problem converging due to the steep gradients in $w_1$. In contrast, Adam and SGD with momentum nicely converge as the changing direction of $w_1$ is canceling itself out. On such surfaces, it is crucial to use momentum."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-XDO7vBrFct"
      },
      "source": [
        "### What optimizer to take\n",
        "\n",
        "After seeing the results on optimization, what is our conclusion? Should we always use Adam and never look at SGD anymore? The short answer: no. There are many papers saying that in certain situations, SGD (with momentum) generalizes better where Adam often tends to overfit [5,6]. This is related to the idea of finding wider optima. For instance, see [Keskar et al., 2017](https://arxiv.org/pdf/1609.04836.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3y02UhprFct"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this tutorial, we have looked at commonly used activation functions, initialization, and optimization techniques for neural networks. We have seen that a good initialization has to balance the preservation of the gradient variance as well as the activation variance. In optimization, concepts like momentum and adaptive learning rate can help with challenging loss surfaces but don't guarantee an increase in performance for neural networks.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "[1] Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of training deep feedforward neural networks.\" Proceedings of the thirteenth international conference on artificial intelligence and statistics. 2010. [link](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)\n",
        "\n",
        "[2] He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level performance on imagenet classification.\" Proceedings of the IEEE international conference on computer vision. 2015. [link](https://www.cv-foundation.org/openaccess/content_iccv_2015/html/He_Delving_Deep_into_ICCV_2015_paper.html)\n",
        "\n",
        "[3] Kingma, Diederik P. & Ba, Jimmy. \"Adam: A Method for Stochastic Optimization.\" Proceedings of the third international conference for learning representations (ICLR). 2015. [link](https://arxiv.org/abs/1412.6980)\n",
        "\n",
        "[4] Keskar, Nitish Shirish, et al. \"On large-batch training for deep learning: Generalization gap and sharp minima.\" Proceedings of the fifth international conference for learning representations (ICLR). 2017. [link](https://arxiv.org/abs/1609.04836)\n",
        "\n",
        "[5] Wilson, Ashia C., et al. \"The Marginal Value of Adaptive Gradient Methods in Machine Learning.\" Advances in neural information processing systems. 2017. [link](https://papers.nips.cc/paper/7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.pdf)\n",
        "\n",
        "[6] Ruder, Sebastian. \"An overview of gradient descent optimization algorithms.\" arXiv preprint. 2017. [link](https://arxiv.org/abs/1609.04747)\n",
        "\n",
        "[7] Lippe, Phillip. UvA Deep Learning Tutorials. https://github.com/phlippe/uvadlc_notebooks"
      ],
      "metadata": {
        "id": "fHOZb6NHOrEC"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}