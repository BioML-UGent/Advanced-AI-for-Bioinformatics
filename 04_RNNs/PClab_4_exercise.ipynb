{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BioML-UGent/Advanced-AI-for-Bioinformatics/blob/main/04_RNNs/PClab_4_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WrQv9yrEbWo3"
      },
      "source": [
        "# RNNs\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Recurrent networks strictly operate on 1-D sequences. They can be used for a variety of tasks, pictured below:\n",
        "\n",
        "<img src=\"http://karpathy.github.io/assets/rnn/diags.jpeg\" width = 500>\n",
        "\n",
        "Examples of the settings in the picture:\n",
        "- one to one: vanilla MLPs that map a fixed size 1-D vector to a 1-D vector for classification or regression\n",
        "- one to many: Image captioning, given an input embedding (obtained with a CNN), a textual caption of variable length is generated.\n",
        "- many to one: (1) Sentence classification such as sentiment analysis or (2) image generation from text: in both cases variable input texts are given as input and a fixed dimensional output is generated.\n",
        "- many to many: (1) machine translation of a variable-length sentence to another variable-length sentence or (2) transcription of a variable-length .mp3 audio to a variable length text.\n",
        "- many to many (1to1 correspondence): (1) Video classification: one label for a variable number of frames in the video (the video frame embedding can be obtained with a CNN and then input into a RNN), (2) autoregressive language modeling: trying to predict the next word in the sentence, for generative purposes or (3) word classification: classify every word as belonging to a category.\n",
        "\n",
        "Note that these settings are not exclusive to recurrent neural networks. In fact, any network type that works on variable input sequences can be used towards these ends. Most famously of which are of course, Transformers, which have all but replaced RNNs in NLP and many other fields.  Other than RNNs and Transformers, convolutional networks can also be used on variable length inputs: a 1D kernel can equally well convolve over a sequence of length $100$ as $1000$. It is only because of the linear layers at the end for classification requiring a specific number of input nodes that typical CNNs become applicable on only one specific input size."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1 - Time sequence prediction"
      ],
      "metadata": {
        "id": "GkxDuKQjz_bI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM\n",
        "Long Short-Term Memory (LSTM) networks are a special type of Recurrent Neural Network (RNN) designed to address the vanishing gradient problem, which makes it difficult for traditional RNNs to learn long-term dependencies in sequential data."
      ],
      "metadata": {
        "id": "uZ-DeARI0KFj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Long Short-Term Memory (LSTM) Networks using PyTorch\n",
        "LSTMs are widely used for sequence modeling tasks because of their ability to capture long-term dependencies. PyTorch provides a clean and flexible API to build and train LSTM models. In PyTorch, the nn.LSTM module handles the recurrence logic, while the rest of the architecture (such as fully connected layers, dropout, etc.) can be customized as needed.\n",
        "\n",
        "Key Components\n",
        "1. Input Size: Number of features in the input sequence at each time step.\n",
        "\n",
        "2. Hidden Size: Number of features in the hidden state.\n",
        "\n",
        "3. Number of Layers: Stacking multiple LSTM layers deepens the model.\n",
        "\n",
        "4. Batch First: If set to True, input/output tensors are provided as (batch, seq_len, features) instead of (seq_len, batch, features).\n",
        "\n",
        "5. Outputs:\n",
        "\n",
        "Output Sequence: Hidden states at each time step.\n",
        "Hidden State: Final hidden state for all layers.\n",
        "Cell State: Final memory cell state for all layers."
      ],
      "metadata": {
        "id": "C1uQDB--07ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Import Libraries and Prepare Data"
      ],
      "metadata": {
        "id": "bljDo8Q81Gm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "ZuKwvbPm0DIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "torch.manual_seed(0)\n",
        "\n",
        "number_of_samples = 1000\n",
        "\n",
        "t = np.linspace(0, 100, number_of_samples)\n",
        "\n",
        "data = np.sin(t)\n",
        "\n",
        "def create_sequences(data, seq_length):\n",
        "    xs, ys = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        x = data[i:(i + seq_length)]\n",
        "        y = data[i + seq_length]\n",
        "        xs.append(x)\n",
        "        ys.append(y)\n",
        "    return np.array(xs), np.array(ys)\\\n",
        "\n",
        "seq_length = 10\n",
        "\n",
        "\n",
        "train_data = data[:int(len(data)/2)]\n",
        "\n",
        "X, y = create_sequences(train_data, seq_length)\n",
        "trainX = torch.tensor(X[:, :, None], dtype=torch.float32)\n",
        "trainY = torch.tensor(y[:, None], dtype=torch.float32)\n",
        "\n",
        "test_data = data[int(len(data)/2):]\n",
        "\n",
        "new_X, new_y = create_sequences(test_data, seq_length)\n",
        "testX = torch.tensor(new_X[:, :, None], dtype=torch.float32)\n",
        "testY = torch.tensor(new_y[:, None], dtype=torch.float32)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for i in range(0, 110, 4):\n",
        "  plt.plot(range(i, seq_length+i), X[i], label='Original Data')"
      ],
      "metadata": {
        "id": "qeuXpaXw2ZYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Define the LSTM Model"
      ],
      "metadata": {
        "id": "ne85G1tN1aPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 1\n",
        " Define an LSTM model"
      ],
      "metadata": {
        "id": "9M8QeQ1P5nrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        ### your code here\n",
        "        # ...\n",
        "        # ...\n",
        "\n",
        "    def forward(self, x, h0=None, c0=None):\n",
        "        ### your code here\n",
        "        # ...\n",
        "        # ...\n",
        "        return out, hn, cn"
      ],
      "metadata": {
        "id": "bo4ps2Am1jD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3: Initialize Model"
      ],
      "metadata": {
        "id": "maQzaHph1jub"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "Initialize your model, as well as loss function and optimizer"
      ],
      "metadata": {
        "id": "8R5TqY8T5_Yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here\n",
        "# ...\n",
        "# ..."
      ],
      "metadata": {
        "id": "ad9aEH-21rhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Train the LSTM Model"
      ],
      "metadata": {
        "id": "kXLECjqV1qoZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3\n",
        "Write a script to train your LSTM."
      ],
      "metadata": {
        "id": "Gyi-UtMH6Lz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here\n",
        "        # ...\n",
        "        # ..."
      ],
      "metadata": {
        "id": "2qVSArsM1vi6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Evaluate and Plot Predictions"
      ],
      "metadata": {
        "id": "T-vWSASw1xeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "predicted, _, _ = model(testX, h0, c0)\n",
        "\n",
        "original = test_data[seq_length:]\n",
        "time_steps = np.arange(seq_length, len(train_data))\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(time_steps, original, label='Original Data')\n",
        "plt.plot(time_steps, predicted.detach().numpy(),\n",
        "         label='Predicted Data', linestyle='--')\n",
        "plt.title('LSTM Model Predictions vs. Original Data')\n",
        "plt.xlabel('Time Step')\n",
        "plt.ylabel('Value')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KnLPO5HZ11SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Noisy data"
      ],
      "metadata": {
        "id": "KTErcCgk4ZSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we'll add uncorrelated Gaussian noise (with variance equal to $\\epsilon$) to the train and test samples, and train our model again."
      ],
      "metadata": {
        "id": "gWnjpmK74hk0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "Generate noisy dataset with $\\epsilon = 0.1$."
      ],
      "metadata": {
        "id": "9NS4exnn4v-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### your code here\n",
        "# ...\n",
        "# ..."
      ],
      "metadata": {
        "id": "tLOQzmTgz-u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 5\n",
        "Re-train the model and look at the results. Try with a greater $\\epsilon$ and discuss the results."
      ],
      "metadata": {
        "id": "yEq_Yvss61kJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFhcwjLBbWo9"
      },
      "source": [
        "## Part 2: Bicycle traffic prediction\n",
        "\n",
        "\n",
        "In this part, we will explore autoregressive modeling on bicycle traffic prediction using measurements from at Campus Coupure.\n",
        "\n",
        "<img src=\"https://images0.persgroep.net/rcs/4cQwm-ofvb3eyIKMWnNf5axxLHg/diocontent/217261403/_fitwidth/694/?appId=21791a8992982cd8da851550a453bd7f&quality=0.8\" width = 500>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArqjiofwdVJx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import urllib.request\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/BioML-UGent/MLLS/main/13_rnns/train_data.csv\", \"./train_data.csv\")\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/BioML-UGent/MLLS/main/13_rnns/test_data.csv\", \"./test_data.csv\")\n",
        "\n",
        "train_data = pd.read_csv(\"train_data.csv\", sep = \",\")\n",
        "test_data = pd.read_csv(\"test_data.csv\", sep = \",\")\n",
        "\n",
        "train_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCTYwonrNbEd"
      },
      "source": [
        "Let's encode the Hour as separate feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqtJ0rjyLZuw"
      },
      "outputs": [],
      "source": [
        "train_data[\"Hour\"] = train_data[\"Date_hour\"].str.split(\"T\", expand = True)[1].astype(float)\n",
        "test_data[\"Hour\"] = test_data[\"Date_hour\"].str.split(\"T\", expand = True)[1].astype(float)\n",
        "train_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45I4eXgtdUrm"
      },
      "source": [
        "\n",
        "\n",
        "Let's take a sequence length of 48 as a default, meaning that our samples will always coincide with two consecutive days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H_xO94-nfEUJ"
      },
      "outputs": [],
      "source": [
        "def generate_batches(sequence, seqlen = 48):\n",
        "    batches = []\n",
        "    for i in np.arange(0, len(sequence) - seqlen, seqlen):\n",
        "        batches.append(sequence[i:i+seqlen])\n",
        "    return torch.stack(batches)\n",
        "\n",
        "train_batches = generate_batches(torch.tensor(train_data[[\"Hour\", \"Totaal\"]].values.astype(np.float32)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DeB-YAajJye"
      },
      "source": [
        "Let's see how a sample looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oV9G5RHzmiW1"
      },
      "outputs": [],
      "source": [
        "train_batches.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqNij14zjIlf"
      },
      "outputs": [],
      "source": [
        "train_batches[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fogv0s--jiXD"
      },
      "source": [
        "We could give our input to the model like this, as the hour is a numerical value which can be interpreted using linear layers. It would make more sense to treat the hour variable as categorical and encode it using dummy variables (one-hot encoding)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1XAhDa7pmdNd"
      },
      "outputs": [],
      "source": [
        "one_hot_hour = torch.nn.functional.one_hot(train_batches[:, :, 0].long())\n",
        "\n",
        "one_hot_hour.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RTNwK3Amv21"
      },
      "outputs": [],
      "source": [
        "train_batches = torch.cat([train_batches[:, :, [1]], one_hot_hour], axis = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5h43ZQWKm3G4"
      },
      "outputs": [],
      "source": [
        "train_batches.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Aw3OsNCom4Ic"
      },
      "outputs": [],
      "source": [
        "train_batches[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kHh7PNFONWN"
      },
      "source": [
        "Let's min-max scale the outputs. Remember: if you don't do this, your loss function (e.g. MSELoss) will be of a very big scale, affecting learning (i.e. you will need lower learning rates).\n",
        "Hence, scaling allows us to make a better guess as to what a good learning rate will be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0fRtX3GOjFQ"
      },
      "outputs": [],
      "source": [
        "max_cyclists_train = train_batches[:, :, 0].max()\n",
        "train_batches[:, :, 0] = train_batches[:, :, 0] / max_cyclists_train\n",
        "train_batches[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lLoumfYPSVE"
      },
      "source": [
        "Finally, let's prepare some batches in a similar way for the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7UOVAf7PUX0"
      },
      "outputs": [],
      "source": [
        "test_batches = generate_batches(torch.tensor(test_data[[\"Hour\", \"Totaal\"]].values.astype(np.float32)))\n",
        "one_hot_hour = torch.nn.functional.one_hot(test_batches[:, :, 0].long())\n",
        "test_batches = torch.cat([test_batches[:, :, [1]], one_hot_hour], axis = 2)\n",
        "test_batches[:, :, 0] = test_batches[:, :, 0] / max_cyclists_train\n",
        "test_batches[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "autQkT0vn_L5"
      },
      "source": [
        "### LSTMs in PyTorch\n",
        "\n",
        "We will again use the LSTM, you can refer to the first part of this exercise for a recap on LSTMs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 6\n",
        "Implement an LSTM model for cycler forecasting by completing the code below. Keep in mind how many input variables we have in our dataset."
      ],
      "metadata": {
        "id": "eI0AHYoRpSEl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "13mJg1lqomei"
      },
      "outputs": [],
      "source": [
        "class CyclerForecaster(nn.Module):\n",
        "    def __init__(self, # ...input_dim =  ..., hidden_dim =  ...):\n",
        "        super().__init__()\n",
        "        self.lstm = # ...\n",
        "        self.output_head = # ...\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # ...\n",
        "\n",
        "        return # ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPgN3P4NpvJL"
      },
      "outputs": [],
      "source": [
        "model = CyclerForecaster()\n",
        "\n",
        "x = torch.randn(2, 48, 25)\n",
        "\n",
        "y = model(x)\n",
        "\n",
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwqJ1fTpnCWa"
      },
      "source": [
        "To create an input and an output, we have to do the above-mentioned shifting. In practice, this means that we take all but the last timepoints to create X, and all but the first timepoints to create Y, hence creating shifted X,y pairs.\n",
        "\n",
        "Additionally, for y, we only want to keep the first variable, meaning the number of cyclists itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XMw9_32bf9hj"
      },
      "outputs": [],
      "source": [
        "X_train = train_batches[:, :-1]\n",
        "\n",
        "y_train = train_batches[:, 1:]\n",
        "y_train = y_train[:, :, [0]]\n",
        "\n",
        "print(X_train.shape, y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhHNqiqHQTiM"
      },
      "source": [
        "Doing the same for the test data, and putting the things into datasets and dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvr5sBhDhR5G"
      },
      "outputs": [],
      "source": [
        "X_test = test_batches[:, :-1]\n",
        "y_test = test_batches[:, 1:, [0]]\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size = 8, shuffle = True)\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size = 8, shuffle = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tM5AD2u7rNA2"
      },
      "source": [
        "Note that we are just taking the first 80% of the data as training and the last 20% as validation set. Normally, we shuffle our data so that we are not biased. In this case, however, we can make a case for doing it our way: because the samples are ordered by day and month, the last samples will be from the summer months, where we may expect different patterns (June: exams, July: vacations)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DmpFl85TvvF0"
      },
      "source": [
        "### Exercise 7\n",
        "Implement the training loop for the Cycler Forecaster using the same principles from last PC labs. You will need to increase the number of epochs as our dataset is quite small and each epoch only constitutes a small number of training steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RclHsVTChfYQ"
      },
      "outputs": [],
      "source": [
        "### your code here\n",
        "# ...\n",
        "# ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XntUGcU0wR8m"
      },
      "source": [
        "To evaluate our model beyond looking at a loss function going down: we can look at the autoregressive results for a random test sample:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYeAfeVhhnCA"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(np.arange(len(y_test[0])), y_test[0])\n",
        "plt.plot(np.arange(len(y_test[0])), model(X_test[[0]]).detach().squeeze(0).numpy())\n",
        "plt.legend([\"True\", \"Predicted\"])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXMxdryOwuY5"
      },
      "source": [
        "\n",
        "\n",
        "So far, at every timestep, we are predicting only one timestep (hour) in advance. To do forecasting for a longer time limit, we should feed the predictions of the model back into the model, like this:\n",
        "\n",
        "\n",
        "To perform this, we will create a helper function that extracts the next step:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SiyoKVzAx_sc"
      },
      "outputs": [],
      "source": [
        "def generate_next_timestep(previous, model):\n",
        "    with torch.no_grad():\n",
        "        output = model(previous.unsqueeze(0))[0, -1]\n",
        "\n",
        "    return output\n",
        "\n",
        "def generate_n_timesteps(previous, model, n = 3):\n",
        "    for _ in range(n):\n",
        "        prediction = generate_next_timestep(previous, model)\n",
        "        # to make a next prediction, we not only need to add the prediction but also the covariates (hour) to our next input:\n",
        "        next_timestep_hour = nn.functional.one_hot((previous[-1, 1:].argmax() + 1) % 24, num_classes = 24)\n",
        "        new_input = torch.cat([prediction, next_timestep_hour])\n",
        "        previous = torch.cat([previous, new_input.unsqueeze(0)])\n",
        "\n",
        "    return previous[:, 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROyU81wA8Ixe"
      },
      "outputs": [],
      "source": [
        "sample_index = 0\n",
        "priming_points = 20\n",
        "generate_steps = 100\n",
        "\n",
        "\n",
        "plt.plot(np.arange(len(X_test[sample_index])), X_test[sample_index, :, 0])\n",
        "predictions = generate_n_timesteps(X_test[sample_index, :priming_points], model, n = generate_steps).numpy()\n",
        "plt.plot(np.arange(len(predictions)), predictions)\n",
        "plt.axvline(x = priming_points-1, color = \"green\", linestyle = \":\")\n",
        "plt.legend([\"True\", \"Predicted\"])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('gaetan_predmod')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "ef9dbff06f2d3990401ab7bd73f166712843564b695d78ee6ca1b88459deb91e"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}