{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BioML-UGent/Advanced-AI-for-Bioinformatics/blob/main/06_calibration/PClab_6_exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evyJ3TNLs1jU"
      },
      "source": [
        "# PC-Lab: Calibration of Machine Learning Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-mm4Gphs1jV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import make_classification, make_blobs\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7ssNofhs1jV"
      },
      "source": [
        "For this tutorial, we'll make use of the python package [pycalib](https://classifier-calibration.github.io/PyCalib/). It first has to installed into your environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFMoyKD8s1jV"
      },
      "outputs": [],
      "source": [
        "!pip install pycalib\n",
        "import pycalib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss16HzmLs1jV"
      },
      "source": [
        "## Introduction to Calibration in Machine Learning\n",
        "\n",
        "In machine learning, uncertainty estimation is crucial for understanding the confidence level of model predictions.\n",
        " It involves quantifying the degree of certainty or doubt associated with a model's predictions.\n",
        "\n",
        "## What is Calibration?\n",
        "\n",
        "Calibration in machine learning refers to the process of aligning a model's predicted probabilities with the actual outcomes.\n",
        " A well-calibrated model provides probability estimates that match the true likelihood of an event occurring.\n",
        " For example, if a calibrated model predicts an event with 70% probability,\n",
        "then approximately 70% of the predictions at this probability level should be correct.\n",
        "\n",
        "### The Importance of Calibration\n",
        "\n",
        "Calibration is essential for two main reasons:\n",
        "\n",
        "- **Trustworthiness**: A calibrated model ensures _trustworthiness_ in its predictions, which is vital for decision-making processes.\n",
        "- **Decision-making**: In some applications, decisions are made based on the predicted probabilities rather than the classification itself. Accurate probability estimates are, therefore, crucial.\n",
        "\n",
        "### Assesing Model Calibration\n",
        "\n",
        "1. **Calibration Curves**\n",
        "\n",
        "<img src=\"https://www.researchgate.net/publication/368304257/figure/fig2/AS:11431281118358760@1675739847164/Example-of-a-calibration-curve-which-plots-the-observed-frequency-of-instances-belonging.ppm\" width=400>\n",
        "\n",
        "2. **Analytical Scores**, i.e. calibration error estimates that estimate confidence, classwise or strong notions of calibration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9KVkWBBss1jV"
      },
      "source": [
        "## 1. Binary dataset\n",
        "\n",
        "In the first task, we will look at the simple case of binary classification, and compare the calibration of different classifier models\n",
        "using calibration curves and the expected calibration error as ann error estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imjaCazCs1jW"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "dataset_binary = make_blobs(n_samples=10000, centers=5, n_features=2, random_state=42)\n",
        "\n",
        "# make the dataset binary\n",
        "dataset_binary[1][:] = dataset_binary[1] > 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANZhT8ois1jW"
      },
      "source": [
        "Visualise the dataset in a scatterplot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYzyrHYhs1jW"
      },
      "outputs": [],
      "source": [
        "x, y = dataset_binary\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "scatter = ax.scatter(x[:, 0], x[:, 1], c=y, cmap='viridis')\n",
        "handles, labels = scatter.legend_elements()\n",
        "labels = ['Class 0', 'Class 1']\n",
        "\n",
        "legend = ax.legend(handles, labels, loc=\"upper right\", title=\"Classes\")\n",
        "ax.add_artist(legend)\n",
        "plt.title('Binary Classification Dataset')\n",
        "ax.set_xlabel('Feature 1')\n",
        "ax.set_ylabel('Feature 2')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F29YjrPMs1jW"
      },
      "source": [
        "### Exercise 1\n",
        "Split your training data into training and validation, use a bigger percentage (50 %) for testing the calibration afterwards. (Why??). Fit a simple Naive Bayes Classifier on your data.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05ET87vOs1jW"
      },
      "outputs": [],
      "source": [
        "# train test split\n",
        "x_train, x_test, y_train, y_test = # ....\n",
        "\n",
        "# classifier\n",
        "clf = # ....\n",
        "clf.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MD7Gz5p6s1jW"
      },
      "source": [
        "_Why choose a higher percentage for the test set_?\n",
        "- higher reliability: more reliable and stable calibration estimates\n",
        "- generalizability\n",
        "- statisitical significance: more statistical power to detect differences/issues in calibration"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 2\n",
        "Visualize the predicted probabilities of your classifier predicted for class 1 in dependence of the feature space using <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.contour.html\" target=\"_top\"> contour lines </a> and histograms. For the latter, use 10 equal-length bins to divide the predicted probabilities.\n"
      ],
      "metadata": {
        "id": "ABPKz3TYtHfs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07pMeWJ8s1jW"
      },
      "outputs": [],
      "source": [
        "# divide the feature space into a grid\n",
        "delta = 0.25\n",
        "x0_grid = np.arange(x[:, 0].min(), x[:, 0].max(), delta)\n",
        "x1_grid = np.arange(x[:, 1].min(), x[:, 1].max(), delta)\n",
        "X0, X1 = np.meshgrid(x0_grid, x1_grid)\n",
        "# predict the probabilities for each point on the grid\n",
        "Y = # ....\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "# do a contour plot of the probabilities for class 1\n",
        "CS = # ax.contour(...)\n",
        "ax.clabel(CS, inline=1, fontsize=15)\n",
        "ax.scatter(x_test[:1000, 0], x_test[:1000, 1], c=y_test[:1000])\n",
        "ax.set_title('Predicted probabilities')\n",
        "ax.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3plepots1jX"
      },
      "source": [
        "Histogram:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93hyTlies1jX"
      },
      "outputs": [],
      "source": [
        "# divide the scores into bins\n",
        "n_bins = 10\n",
        "bins = # ...\n",
        "\n",
        "# predictions on test set\n",
        "p_pred = # ...\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "\n",
        "ax.hist([p_pred[y_test == 0, 1], p_pred[y_test == 1, 1]], bins=bins, orientation='horizontal', label=['Class 0', 'Class 1'],\n",
        "        color=[\"purple\", \"yellow\"])\n",
        "ax.set_ylim(0, 1)\n",
        "ax.grid()\n",
        "ax.set_xlabel('Number of samples per bin')\n",
        "ax.set_ylabel('Predicted probability')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmClUVn7s1jX"
      },
      "source": [
        "We see that the model gives lower scores to the first class, and that most of the instances of that class\n",
        "are in the range of scores between $0$ and $0.1$. The last bin between $0.9$ and $1.0$ does not contain any sample."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 3\n",
        "Plot a reliability diagram. In order to do this, use a number of B=10 bins to divide the predicted probabilities Use the <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.digitize.html\" target=\"_top\"> numpy.digitize </a> function to return the index of the corresponding bin for each instance. Compute the mean score for each bin and compare the true positive rate in each bin (i.e. the percentage of instances classified as 1).\n"
      ],
      "metadata": {
        "id": "4bdV-PNatXWH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j65gxRAOs1jX"
      },
      "outputs": [],
      "source": [
        "# assign each of the instances to a bin\n",
        "bin_index = #....\n",
        "\n",
        "# compute the mean score in each bin\n",
        "pred_means = # ....\n",
        "\n",
        "y_means = # ...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbFQC-DDs1jX"
      },
      "source": [
        "Now we can plot a line with the mean scores in the $x$ axis and the true positive rate on the $y$ axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYeanEwhs1jX"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots( figsize=(7, 5))\n",
        "ax.plot(pred_means, y_means, 'o-')\n",
        "ax.plot([0, 1], [0, 1], 'r--')\n",
        "ax.set_xlim(0, 1)\n",
        "ax.set_xlabel('Mean predicted value')\n",
        "ax.set_ylim(0, 1)\n",
        "ax.set_ylabel('Fraction of positives')\n",
        "ax.set_title('Calibration curve for class 1')\n",
        "ax.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDgDXg60s1jX"
      },
      "source": [
        "We see that the model is\n",
        "\n",
        "- _over-confident_ in the lower range of scores: predicts scores of 0.2 when the true proportion of positives is around 10%.\n",
        "- _under-confident_ in the higher range of scores: predicts scores of 0.8 when the true proportion of positives is 100 %."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4\n",
        "Train a KNN classifier on the data and compare the calibration to the NaiveBayes in a joint calibration curve plot. Use the pycalib.visualisations functions plot_reliability_diagram and plot_binary_reliability_diagram_gaps.\n"
      ],
      "metadata": {
        "id": "NchXRGGfth5I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8gTRT-Qs1jX"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "clf2 = # ...\n",
        "clf2.fit(#....)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6v5E563fs1jX"
      },
      "source": [
        "In the PyCalib library we can directly call a function to plot the reliability diagram for a list of output scores. Use the function [```plot_reliability_diagrams```](https://classifier-calibration.github.io/PyCalib/api/visualisations.html#pycalib.visualisations.plot_reliability_diagram) to visualize the calibration curve for both classifiers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrfD4nrMs1jX"
      },
      "outputs": [],
      "source": [
        "from pycalib.visualisations import plot_reliability_diagram\n",
        "\n",
        "scores_list = # ....\n",
        "labels_list = ['KNN', 'GaussianNB']\n",
        "\n",
        "# plot the reliability diagram\n",
        "#\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLxKgar_s1jX"
      },
      "source": [
        "We can also compute the expected calibration error fro both classifiers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gbfy27u3s1jX"
      },
      "outputs": [],
      "source": [
        "from pycalib.metrics import ECE\n",
        "\n",
        "ece_list = [ECE(y_test, scores) for scores in scores_list]\n",
        "print(f\"ECE for KNN: {ece_list[0]:.3f}\")\n",
        "print(f\"ECE for GaussianNB: {ece_list[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VC3EZn-Ks1jX"
      },
      "source": [
        "## 2. Multiclass Setting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YfI7wjFns1jY"
      },
      "source": [
        "Let's do the same analysis for a multiclass problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96EThsI2s1jY"
      },
      "outputs": [],
      "source": [
        "n_features = 7\n",
        "dataset_ternary = make_classification(n_classes=3, n_samples=10000, n_clusters_per_class=3, n_features=n_features, n_informative=5, random_state=42)\n",
        "\n",
        "# visualize the dataset\n",
        "x, y = dataset_ternary\n",
        "fig, ax = plt.subplots(7, 7, figsize=(12, 10))\n",
        "\n",
        "for i in range(n_features):\n",
        "    for j in range(n_features):\n",
        "        ax[i, j].scatter(x[:, i], x[:, j], c=y, cmap='viridis')\n",
        "        ax[i, j].set_xticks([])\n",
        "        ax[i, j].set_yticks([])\n",
        "        ax[i, j].set_xlabel(f'Feature {i}')\n",
        "        ax[i, j].set_ylabel(f'Feature {j}')\n",
        "\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehb7mVi5s1jY"
      },
      "source": [
        "### Exercise 5\n",
        "Again train a Naive Bayes classifier and a k-nearest neighbor classifier with n=10 neighbors on your dataset. Plot the reliability curves for both of them. How do you evaluate calibration in the multi-class scenario?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyBOPe30s1jY"
      },
      "outputs": [],
      "source": [
        "x_train, x_test, y_train, y_test = # ....\n",
        "\n",
        "# define and fit classifiers\n",
        "clf = # ... Naive Bayes\n",
        "# clf.fit(...)\n",
        "clf2 = #  k-nearest neighbor\n",
        "# clf2.fit(...)\n",
        "\n",
        "scores_list = [clf.predict_proba(x_test),\n",
        "          clf2.predict_proba(x_test)]\n",
        "\n",
        "fig = plt.figure(figsize=(10,6 ))\n",
        "\n",
        "# plot the reliability curve\n",
        "# _ = plot_reliability_diagram(....)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 6\n",
        "Calculate confidence and classwise ECE for both classifiers using the pre-implemented functions from pycalib.\n"
      ],
      "metadata": {
        "id": "U5gy26S3vDWb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHF6tOygs1jY"
      },
      "outputs": [],
      "source": [
        "from pycalib.metrics import conf_ECE, classwise_ECE\n",
        "\n",
        "# your code\n",
        "#\n",
        "#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MzF4bXyDs1jY"
      },
      "source": [
        "## 3. Calibration Maps: Isotonic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UwrbIwL0s1jY"
      },
      "source": [
        "In the last section, we will look at two ways of calibrating the predictions of a model after it has been trained.\n",
        "\n",
        "**Platt Scaling**, also known as logistic calibration, fits a logistic regression model to the classifier's output. In particular,\n",
        "\n",
        "having a model $f$, the transformation $$P(y=1|x) = \\frac{1}{1 + \\exp(Af(x) + B)}$$ is used to logistically transform the scores.\n",
        "\n",
        "On the other hand, **Isotonic Regression** works by fitting a non-decreasing functiion to the model's outputs, which is learned by minimizing a least-squares criterion on the calibration data.\n",
        "\n",
        "\n",
        "\n",
        "We will load again the binary dataset that we created earlier, and use the Naive BAyes Classifier to compare the two methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSjPsRnxs1ji"
      },
      "outputs": [],
      "source": [
        "x, y = dataset_binary\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Y3kJkQs1ji"
      },
      "source": [
        "### Exercise 7\n",
        "\n",
        "This time, use the sklearn class <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html\" target=\"_top\">CalibratedClassifierCV </a> to train three different classifiers: the uncalibrated naive Bayes, the naive Bayes with Platt scaling calibration and the naive Bayes with isotonic regression calibration.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSH6Wu3Ts1ji"
      },
      "outputs": [],
      "source": [
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "\n",
        "clf = # ....   uncalibrated naive Bayes\n",
        "clf.fit(x_train, y_train)\n",
        "\n",
        "cal_clf_1 = # ....   calibrated naive Bayes - platt scaling\n",
        "cal_clf_1.fit(x_train, y_train)\n",
        "\n",
        "cal_clf_2 = # ....   calibrated naive Bayes - isotonic regression\n",
        "cal_clf_2.fit(x_train, y_train)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 8\n",
        "Visualize the reliabiltiy diagram of all three classifiers.\n"
      ],
      "metadata": {
        "id": "7wUpcfXcvX7b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fibMHk1ms1jj"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 6))\n",
        "\n",
        "# _ = plot_reliability_diagram(...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "132az_pGs1jj"
      },
      "source": [
        "#### Comparison of the two calibration maps\n",
        "\n",
        "Isotonic Regression is closer to the diagonal line."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}